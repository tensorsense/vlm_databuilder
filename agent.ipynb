{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "_ = load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import TypedDict, Annotated, Sequence, List, Optional\n",
    "import operator\n",
    "\n",
    "from langchain_core.messages import BaseMessage, HumanMessage, SystemMessage\n",
    "from langchain.pydantic_v1 import BaseModel, Field"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import AzureChatOpenAI\n",
    "\n",
    "llm = AzureChatOpenAI(\n",
    "    temperature=0.0,\n",
    "    azure_deployment=\"gpt4o\",\n",
    "    openai_api_version=\"2023-07-01-preview\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VideoInfo(BaseModel):\n",
    "    video_id: str\n",
    "    url: str\n",
    "    relative_video_path: str\n",
    "    subs: str\n",
    "    transcript: str\n",
    "\n",
    "\n",
    "class SegmentInfo(BaseModel):  # , Generic[OutputSchema]):\n",
    "    start_timestamp: str\n",
    "    end_timestamp: str\n",
    "    fps: float\n",
    "    # segment_info: Optional[OutputSchema]\n",
    "    video_id: str\n",
    "    # _frames: Optional[\n",
    "    # list[np.array]\n",
    "    # ]  # List of raw frames that got into LLM. Added for debugging purposes.\n",
    "\n",
    "    # @classmethod\n",
    "    # def from_frames(cls, start_frame, end_frame, fps, **kwargs):\n",
    "    #     return cls(\n",
    "    #         start_timestamp=seconds_to_ts(start_frame / fps),\n",
    "    #         end_timestamp=seconds_to_ts(end_frame / fps),\n",
    "    #         fps=fps,\n",
    "    #         **kwargs,\n",
    "    #     )\n",
    "\n",
    "    @classmethod\n",
    "    def from_seconds(cls, start_seconds, end_seconds, **kwargs):\n",
    "        return cls(\n",
    "            start_timestamp=seconds_to_ts(start_seconds),\n",
    "            end_timestamp=seconds_to_ts(end_seconds),\n",
    "            **kwargs,\n",
    "        )\n",
    "\n",
    "    # def to_str(self, skip: list[str] = []):\n",
    "    #     # skip -> fields from segment_info\n",
    "    #     # dict() works both with pydantic model and with with unparsed dict\n",
    "    #     if self.segment_info:\n",
    "    #         d = dict(self.segment_info)\n",
    "    #         for s in skip:\n",
    "    #             del d[s]\n",
    "    #         d = \": \" + json.dumps(d)\n",
    "    #     else:\n",
    "    #         d = \"\"\n",
    "    #     return f\"{self.start_timestamp}-{self.end_timestamp}{d}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Create the state\n",
    "\n",
    "class AgentState(TypedDict):\n",
    "\ttask: str\n",
    "\tsearch_queries: List[str]\n",
    "\tvideo_ids: List[str]\n",
    "\tvideo_infos: List[VideoInfo]\n",
    "\tclip_text_prompts: List[str] = [\"person doing squats\"]\n",
    "\tsegment_infos: List[SegmentInfo]\n",
    "\tclues = List[str]\n",
    "\tannotations = List[str]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Set prompts\n",
    "\n",
    "GEN_QUERIES_PROMPT = (\n",
    "    \"You a helping the user to find a very large and diverse set of videos on a video hosting service.\",\n",
    "    \"A user will only describe which videos they are looking for and how many queries they need.\",\n",
    ")\n",
    "\n",
    "# prompt='I want to find instructional videos about how to do squats.',\n",
    "# num_queries_prompt = f'I need {num_queries} queries'\n",
    "\n",
    "EXTRACT_CLUES_PROMPT = \"\"\"You are a highly intelligent data investigator.  \n",
    "You take unstructured damaged data and look for clues that could help restore the initial information\n",
    "and extract important insights from it.\n",
    "You are the best one for this job in the world because you are a former detective. \n",
    "You care about even the smallest details, and your guesses about what happened in the initial file\n",
    "even at very limited inputs are usually absolutely right.  \n",
    "You use deductive and inductive reasoning at the highest possible quality.\n",
    "\n",
    "#YOUR TODAY'S JOB\n",
    "The user needs to learn about what happens in a specific segment of a video file. Your job is to help the user by providing clues that would help the user make the right assumption.\n",
    "The user will provide you with: \n",
    "1. Instructions about what kind of information the user is trying to obtain.\n",
    "2. A list of time codes of the segments in format \"<HH:MM:SS.ms>-<HH:MM:SS.ms>\". All the provided segment of the video contain what the user is looking for, but other parts of the video might have different content.\n",
    "3. A transcript of the *full video* in format of \"<HH.MM.SS>\\\\n<text>\"\n",
    "\n",
    "Your task:\n",
    "1. Read the transcript.\n",
    "2. Provide the clues in a given format.\n",
    "3. Provied any other info requested by the user.\n",
    "\n",
    "#RULES\n",
    "!!! VERY IMPORTANT !!!\n",
    "1. Rely only on the data provided in the transcript. Do not improvise. All the quotes and corresponding timestamps must be taken from the transcript. Quote timestamps must be taken directly from the transcript.\n",
    "2. Your job is to find the data already provided in the transcript.\n",
    "3. Analyze every segment. Only skip a segment if there is no information about it in the trascript.\n",
    "4. For local clues, make sure that the quotes that you provide are located inside the segment. To do this, double check the timestamps from the transcript and the segment.\n",
    "5. For all clues, make sure that the quotes exactly correspond to the timestamps that you provide.\n",
    "6. When making clues, try as much as possible to make them describe specifically what is shown in the segment.\n",
    "7. Follow the format output.\n",
    "8. Be very careful with details. Don't generalize. Always double check your results.\n",
    "\n",
    "Please, help the user find relevant clues to reconstruct the information they are looking for, for each provided segment.\n",
    "\n",
    "WHAT IS A CLUE: A *clue*, in the context of reconstructing narratives from damaged data, \n",
    "is a fragment of information extracted from a corrupted or incomplete source that provides \n",
    "insight into the original content. These fragments serve as starting points for inference \n",
    "and deduction, allowing researchers to hypothesize about the fuller context or meaning of \n",
    "the degraded material. The process of identifying and interpreting clues involves both objective analysis of the \n",
    "available data and subjective extrapolation based on domain knowledge, contextual understanding, \n",
    "and logical reasoning.\n",
    "\n",
    "Here is what the user expects to have from you:\n",
    "1. *Local clues* that would help the user undestand how the thing they are looking for happens inside the segment. Local clues for a segment are generated from quotes inside a specific segment.\n",
    "2. *Global clues* that would help the user understand how the thing they are looking for happens inside the segment. Global clues for a segment are generated from quotes all around the video, but are very relevant to the specific that they are provided for.\n",
    "3. *Logical inferences* that could help the user understand how the thing they are looking for happens inside the segment. Logical inferences for a segment are deducted from local and global clues for this segment.\n",
    "\n",
    "!!!IT IS EXTREMELY IMPORTANT TO DELIVER ALL THREE THINGS!!!\n",
    "\"\"\"\n",
    "\n",
    "# also MANY a structured output prompt\n",
    "\n",
    "# EXTRACT_CLUES_PROMPT = \"\"\"\n",
    "# \"User's instructions: The provided video is a tutorial about how to perform squats.\n",
    "\n",
    "# I need to understand HOW THE PERSON SHOWN IN EACH SEGMENT PERFORMS SQUATS IN THIS SEGMENT.\n",
    "# What is done correctly.\n",
    "# What mistakes they make. Why these mistakes happen.\n",
    "# How these mistakes could be improved.\n",
    "\n",
    "# It is very improtant that the information that you provide would describe how the person shown in the segment is doing squats, and not some generic advice that is unrelated to the visual information.\n",
    "# \"\"\"\n",
    "\n",
    "# prompt.append('Segment timecodes and optional additional information:\\n' + '\\n'.join([s.to_str(skip=[filter_by] if filter_by else []) for s in video_segments_part]))\n",
    "# prompt.append('Transcript:\\n' + transcript)\n",
    "\n",
    "\n",
    "GEN_ANNOTATIONS_PROMPT = \"\"\"You are a helpful assistant that performs high quality data investigation and transformation.\n",
    "                You will be given a JSON object with clues and other helpful information about what's going on \n",
    "                in a specific part of a video file. This part is called a segment. Your job is to:\n",
    "                1. Read this JSON object carefully\n",
    "                2. Answer user's questions about this segment\n",
    "                3. Provide the answer as a JSON object in a schema provided by the user\n",
    "                Important rules:\n",
    "                1. You can only rely on data presented in a provided JSON object. Don't improvise.\n",
    "                2. Follow user's request carefully.\n",
    "                3. Don't rush to deliver the answer. Take some time to think. Make a deep breath. Then start writing.\n",
    "                4. If you want to output field as empty (null), output it as JSON null (without quotes), not as a string \"null\". \n",
    "\"\"\"\n",
    "\n",
    "\n",
    "# human_prompt = \"\"\"\n",
    "# You are given a JSON object that contains clues about segments of a video with timecodes.\n",
    "# !!!! For each segment provided in a JSON object you need to answer on the following questions:\n",
    "# 1. Given the data found in the JSON object, what is a probability that this part contains a footage of a person doing squats? [the answer could be only \"high\", \"medium\", \"low\", or null (if impossible to infer from the provided data)]\n",
    "# 2. Given the data found in the JSON object and even if the answer on the previous question is \"low\", does this person do squats right, wrong, or mixed? [the answer could be only \"right\", \"wrong\", \"mixed\", or null (if impossible to infer from the provided data)]\n",
    "# 3. Given the data found in the JSON object, what exactly does thing person do right and/or wrong regarding their squats technique? [the answer should be clear and focused on body parts]\n",
    "# 4. If the answer on the previous question contains description of wrong technique, explain how to fix these mistakes using your \"own knowledge\" like you are a sports coach.\n",
    "# \"\"\"\n",
    "\n",
    "# for clue in clues_part:\n",
    "#     prompt.append(\"Segment:\\n\" + json.dumps(clue))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from datagen import DatagenConfig, get_video_ids, download_videos, detect_segments_clip, generate_clues, generate_annotations\n",
    "\n",
    "# config_params = {\n",
    "#     \"openai\": {\n",
    "#         \"type\": \"azure\",  # openai/azure\n",
    "#         \"temperature\": \"1\",\n",
    "#         \"deployment\": \"gpt4o\",  # model for openai / deployment for azure\n",
    "#     },\n",
    "#     \"data_dir\": \"./tmp/squats\",\n",
    "# }\n",
    "\n",
    "# !mkdir -p {config_params[\"data_dir\"]}\n",
    "\n",
    "# # this config handles all the bookeeping so you need to pass it everywhere.\n",
    "# config = DatagenConfig(**config_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scrapetube\n",
    "import yt_dlp\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "from collections import defaultdict\n",
    "from datagen.core.sub_utils import vtt_to_txt\n",
    "from datagen.detect_segments import get_segments\n",
    "import torch\n",
    "from transformers import AutoModel, AutoProcessor\n",
    "import pandas as pd\n",
    "from tsmoothie.smoother import LowessSmoother"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import decord\n",
    "import math\n",
    "import numpy as np\n",
    "\n",
    "decord.bridge.set_bridge(\"torch\")\n",
    "\n",
    "\n",
    "class VideoInferenceDataset(torch.utils.data.IterableDataset):\n",
    "    def __init__(self, video_infos: List[VideoInfo]):\n",
    "        super(VideoInferenceDataset).__init__()\n",
    "\n",
    "        self.video_infos = video_infos\n",
    "        self.frame_generator = self.get_frame_generator(video_infos)\n",
    "\n",
    "    @staticmethod\n",
    "    def get_frame_generator(video_infos):\n",
    "\n",
    "        for video_info in video_infos:\n",
    "            video_path = Path(video_info.relative_video_path)\n",
    "            vr = decord.VideoReader(str(video_path))\n",
    "            num_frames = len(vr)\n",
    "            fps = math.ceil(num_frames / video_info.duration)\n",
    "            frame_indices = range(0, num_frames, fps)\n",
    "\n",
    "            for frame_idx in frame_indices:\n",
    "                frame = vr[frame_idx].asnumpy()\n",
    "                yield {\n",
    "                    \"frame\": frame,\n",
    "                    \"frame_idx\": frame_idx,\n",
    "                    \"video_id\": video_info.video_id,\n",
    "                }\n",
    "\n",
    "    def __next__(self):\n",
    "        return next(self.frame_generator)\n",
    "\n",
    "    def __iter__(self):\n",
    "        # worker_info = torch.utils.data.get_worker_info()\n",
    "        return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Create nodes\n",
    "\n",
    "\n",
    "def gen_queries_node(state: AgentState):\n",
    "    class QueryList(BaseModel):\n",
    "        \"\"\"A list of queries to find videos on a video hosting service\"\"\"\n",
    "\n",
    "        search_queries: list[str] = Field(default=None, description=\"a list of queries\")\n",
    "\n",
    "    messages = [\n",
    "        SystemMessage(content=str(GEN_QUERIES_PROMPT)),\n",
    "        HumanMessage(content=state[\"task\"]),\n",
    "    ]\n",
    "\n",
    "    model = llm.with_structured_output(QueryList)\n",
    "    response: QueryList = model.invoke(messages)\n",
    "\n",
    "    return {\"search_queries\": response.search_queries[:2]}\n",
    "\n",
    "\n",
    "def get_video_ids_node(state: AgentState):\n",
    "\n",
    "    queries = state[\"search_queries\"]\n",
    "    videos_per_query = 1\n",
    "    sleep = 0\n",
    "    sort_by = \"relevance\"\n",
    "    results_type = \"video\"\n",
    "    only_creative_commons = False\n",
    "\n",
    "    video_ids = set()\n",
    "    for query in queries:\n",
    "        for video in scrapetube.get_search(\n",
    "            query=query,\n",
    "            limit=videos_per_query,\n",
    "            sleep=sleep,\n",
    "            sort_by=sort_by,\n",
    "            results_type=results_type,\n",
    "        ):\n",
    "            video_ids.add(video[\"videoId\"])\n",
    "    video_ids = list(video_ids)\n",
    "\n",
    "    if only_creative_commons:\n",
    "        video_ids_cc = []\n",
    "        for i in video_ids:\n",
    "            YDL_OPTIONS = {\n",
    "                \"quiet\": True,\n",
    "                \"simulate\": True,\n",
    "                \"forceurl\": True,\n",
    "            }\n",
    "            with yt_dlp.YoutubeDL(YDL_OPTIONS) as ydl:\n",
    "                info = ydl.extract_info(f\"youtube.com/watch?v={i}\", download=False)\n",
    "            if \"creative commons\" in info.get(\"license\", \"\").lower():\n",
    "                video_ids_cc.append(i)\n",
    "        video_ids = video_ids_cc\n",
    "\n",
    "    return {\"video_ids\": video_ids}\n",
    "\n",
    "\n",
    "def download_node(state: AgentState):\n",
    "\n",
    "    LOCAL_ROOT = Path(\"./tmp/agent_squats\").resolve()\n",
    "    video_dir = LOCAL_ROOT / \"videos\"\n",
    "    sub_dir = LOCAL_ROOT / \"subs\"\n",
    "\n",
    "    discard_path = LOCAL_ROOT / \"videos_without_subs\"\n",
    "    discard_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    video_ids = state[\"video_ids\"]\n",
    "\n",
    "    downloaded_video_ids = [video_path.name for video_path in video_dir.glob(\"*.mp4\")]\n",
    "    downloaded_video_ids += [\n",
    "        video_path.name for video_path in discard_path.glob(\"*.mp4\")\n",
    "    ]\n",
    "\n",
    "    only_with_transcripts = True\n",
    "\n",
    "    YDL_OPTIONS = {\n",
    "        \"writeautomaticsub\": True,\n",
    "        \"subtitleslangs\": [\"en\"],\n",
    "        \"subtitlesformat\": \"vtt\",\n",
    "        \"overwrites\": False,\n",
    "        \"format\": \"mp4\",\n",
    "        \"outtmpl\": {\n",
    "            \"default\": video_dir.as_posix() + \"/%(id)s.%(ext)s\",\n",
    "            \"subtitle\": sub_dir.as_posix() + \"/%(id)s\",\n",
    "        },\n",
    "    }\n",
    "\n",
    "    video_infos = []\n",
    "\n",
    "    with yt_dlp.YoutubeDL(YDL_OPTIONS) as ydl:\n",
    "        for video_id in video_ids:\n",
    "            if video_id not in downloaded_video_ids:\n",
    "                try:\n",
    "                    url = f\"https://www.youtube.com/watch?v={video_id}\"\n",
    "                    ydl.download(url)\n",
    "                except Exception as e:\n",
    "                    print(datetime.now(), f\"Error at video {video_id}, skipping\")\n",
    "                    print(datetime.now(), e)\n",
    "                    continue\n",
    "\n",
    "            video_path = Path(ydl.prepare_filename({\"id\": video_id}))\n",
    "            sub_path = Path(ydl.prepare_filename({\"id\": video_id, \"ext\": \"en.vtt\"}))\n",
    "\n",
    "            with sub_path.open(\"r\") as f:\n",
    "                subs = f.read()\n",
    "\n",
    "            transcript = vtt_to_txt(sub_path)\n",
    "\n",
    "            video_info = VideoInfo.from_local_download(\n",
    "                video_id=video_id,\n",
    "                url=url,\n",
    "                video_path=video_path.relative_to(LOCAL_ROOT).as_posix(),\n",
    "                subs=subs,\n",
    "                transcript=transcript,\n",
    "            )\n",
    "\n",
    "            video_infos.append(video_info)\n",
    "\n",
    "    if only_with_transcripts:\n",
    "        filtered_video_infos = []\n",
    "        for video_info in video_infos:\n",
    "            if video_info.transcript:\n",
    "                filtered_video_infos.append(video_info)\n",
    "            else:\n",
    "                video_path = LOCAL_ROOT / video_info.video_path\n",
    "                video_path.rename(discard_path / video_path.name)\n",
    "        video_infos = filtered_video_infos\n",
    "\n",
    "    return {\"video_infos\": video_infos}\n",
    "\n",
    "\n",
    "def detect_segments_node(state: AgentState):\n",
    "\n",
    "    clip_text_prompts = state[\"clip_text_prompts\"]\n",
    "    video_infos = state[\"video_infos\"]\n",
    "\n",
    "    CLIP_MODEL_ID = \"google/siglip-so400m-patch14-384\"\n",
    "\n",
    "    model = AutoModel.from_pretrained(CLIP_MODEL_ID)\n",
    "    processor = AutoProcessor.from_pretrained(CLIP_MODEL_ID)\n",
    "\n",
    "    dataset = VideoInferenceDataset(video_infos)\n",
    "    dataloader = torch.utils.data.DataLoader(dataset, num_workers=0, batch_size=100)\n",
    "\n",
    "    smoother = LowessSmoother(smooth_fraction=0.02, iterations=1)\n",
    "\n",
    "    clip_results_dict = defaultdict(list)\n",
    "\n",
    "    while True:\n",
    "        try:\n",
    "            batch = next(iter(dataloader))\n",
    "        except StopIteration:\n",
    "            break\n",
    "\n",
    "        frames = batch[\"frame\"].to(\"cuda\")\n",
    "\n",
    "        inputs = processor(\n",
    "            images=frames,\n",
    "            text=clip_text_prompts,\n",
    "            return_tensors=\"pt\",\n",
    "            padding=True,\n",
    "            truncation=True,\n",
    "        )\n",
    "\n",
    "        logits = model(**inputs)\n",
    "        probs = torch.nn.functional.sigmoid(logits)\n",
    "\n",
    "        for sample, prob in zip(batch, probs):\n",
    "            video_id = sample[\"video_id\"]\n",
    "            frame_idx = sample[\"frame_idx\"]\n",
    "            clip_results_dict[\"video_id\"].append(video_id)\n",
    "            clip_results_dict[\"frame_idx\"].append(frame_idx)\n",
    "            clip_results_dict[\"probs\"].append(prob.item())\n",
    "\n",
    "    clip_results = pd.DataFrame(clip_results_dict)\n",
    "\n",
    "    max_gap_seconds = 1\n",
    "    fps_sampling = 1\n",
    "    min_prob = 0.1\n",
    "    min_segment_seconds = 3\n",
    "    fps = 25\n",
    "\n",
    "    for video_clip_results in clip_results.groupby(\"video_id\"):\n",
    "        probs = video_clip_results[\"probs\"].values\n",
    "        probs = smoother.smooth(probs).smooth_data[0]\n",
    "        segments_start_end = get_segments(\n",
    "            probs,\n",
    "            max_gap=round(max_gap_seconds * fps_sampling),\n",
    "            min_prob=min_prob,\n",
    "            min_segment=round(min_segment_seconds * fps_sampling),\n",
    "        )\n",
    "        segments = []\n",
    "        for start, end in segments_start_end:\n",
    "            segments.append(\n",
    "                SegmentInfo.from_seconds(\n",
    "                    start,\n",
    "                    end,\n",
    "                    fps=fps,\n",
    "                    video_id=\"a\",\n",
    "                )\n",
    "            )\n",
    "\n",
    "    return {\"segments\": segments}\n",
    "\n",
    "\n",
    "# def extract_clues_node(state: AgentState):\n",
    "#     clues = []\n",
    "\n",
    "#     clues = generate_clues(\n",
    "#         # video_ids=['byxWus7BwfQ'],\n",
    "#         config=config,\n",
    "#         human_prompt=human_prompt,\n",
    "#         segments_per_call=5,  # the output might be quite long, so need to limit number of segments per gpt call to respect max output legnth\n",
    "#         raise_on_error=True,  # interrupt when encountering an error. Useful for debugging.\n",
    "#     )\n",
    "\n",
    "#     return {\"clues\": clues}\n",
    "\n",
    "\n",
    "# def gen_annotations_node(state: AgentState):\n",
    "\n",
    "#     class SegmentFeedback(BaseModel):\n",
    "#         \"\"\"\n",
    "#         —> GOOD EXAMPLES:\n",
    "#             \"wrong\":\"Knees caving in: This can stress the knees and reduce effectiveness\"\n",
    "#             \"correction\":\"Focus on keeping knees aligned with your toes.\"\n",
    "#             \"wrong\":\"Rounding the back: This increases the risk of back injuries\"\n",
    "#             \"correction\":\"Keep your chest up and maintain a neutral spine throughout the movement.\"\n",
    "#             \"wrong\":\"Heels are lifting off the ground: this shifts the weight forward, reducing stability\"\n",
    "#             \"correction\":\" Keep your weight on your heels and press through them as you rise.\"\n",
    "#             \"right\":\"Chest and shoulders: The chest is up, and the shoulders are back, maintaining an upright torso.\"\n",
    "#             \"correction\":null\n",
    "#         —> BAD EXAMPLES:\n",
    "#             \"wrong\":\"knees\"\n",
    "#             \"correction\":\"fix knees\"\n",
    "#             \"wrong\":\"back looks funny\"\n",
    "#             \"correction\":\"make back better\"\n",
    "#             \"wrong\":\"feet are doing something\"\n",
    "#             \"correction\":\"feet should be different\"\n",
    "#             \"right\":\"arms\"\n",
    "#             \"correction\":\"arms are fine i think\"\n",
    "#         —> BAD EXAMPLES END HERE\n",
    "#         \"\"\"\n",
    "\n",
    "#         right: Optional[str] = Field(description=\"what was right in the performance\")\n",
    "#         wrong: Optional[str] = Field(description=\"what was wrong in the performance\")\n",
    "#         correction: Optional[str] = Field(\n",
    "#             description=\"how and in what ways it the performance could be improved\"\n",
    "#         )\n",
    "\n",
    "#     # The segment timestamps are taken from the provided information.\n",
    "#     class SegmentAnnotation(BaseModel):\n",
    "#         squats_probability: Optional[str] = Field(\n",
    "#             description=\"how high is the probability that the person is doing squats in the segment: low, medium, high, unknown(null)\"\n",
    "#         )\n",
    "#         squats_technique_correctness: Optional[str] = Field(\n",
    "#             description=\"correctness of the squat technique.\"\n",
    "#         )\n",
    "#         squats_feedback: Optional[SegmentFeedback] = Field(\n",
    "#             description=\"what was right and wrong in the squat perfomance in the segment. When the technique is incorrect, provide instructions how to correct them.\"\n",
    "#         )\n",
    "\n",
    "#     annotations = generate_annotations(\n",
    "#         human_prompt=human_prompt,\n",
    "#         config=config,\n",
    "#         segments_per_call=5,\n",
    "#         annotation_schema=SegmentAnnotation,\n",
    "#     )\n",
    "\n",
    "#     return {\"annotations\": annotations}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import StateGraph, END\n",
    "from typing import TypedDict, Annotated, List\n",
    "import operator\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "\n",
    "from langchain_core.messages import AnyMessage, SystemMessage, HumanMessage, AIMessage, ChatMessage\n",
    "\n",
    "memory = MemorySaver()\n",
    "# memory = SqliteSaver.from_conn_string(\":memory:\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "builder = StateGraph(AgentState)\n",
    "\n",
    "builder.add_node(\"generate_queries\", gen_queries_node)\n",
    "builder.add_node(\"get_video_ids\", get_video_ids_node)\n",
    "builder.add_node(\"download\", download_node)\n",
    "builder.add_node(\"detect_segments\", detect_segments_node)\n",
    "# builder.add_node(\"extract_clues\", extract_clues_node)\n",
    "# builder.add_node(\"gen_annotations\", gen_annotations_node)\n",
    "\n",
    "builder.set_entry_point(\"generate_queries\")\n",
    "\n",
    "# builder.add_conditional_edges(\n",
    "#     \"generate\", \n",
    "#     should_continue, \n",
    "#     {END: END, \"reflect\": \"reflect\"}\n",
    "# )\n",
    "\n",
    "builder.add_edge(\"generate_queries\", \"get_video_ids\")\n",
    "builder.add_edge(\"get_video_ids\", \"download\")\n",
    "builder.add_edge(\"download\", \"detect_segments\")\n",
    "builder.add_edge(\"detect_segments\", END)\n",
    "\n",
    "# builder.add_edge(\"detect_segments\", \"extract_clues\")\n",
    "# builder.add_edge(\"extract_clues\", \"gen_annotations\")\n",
    "# builder.add_edge(\"gen_annotations\", END)\n",
    "\n",
    "graph = builder.compile(checkpointer=memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'generate_queries': {'search_queries': ['how to do squats', 'squat exercise tutorial']}}\n",
      "{'get_video_ids': {'video_ids': ['xqvCmoLULNY', 'IB_icWRzi4E']}}\n",
      "[youtube] Extracting URL: https://www.youtube.com/watch?v=xqvCmoLULNY\n",
      "[youtube] xqvCmoLULNY: Downloading webpage\n",
      "[youtube] xqvCmoLULNY: Downloading ios player API JSON\n",
      "[youtube] xqvCmoLULNY: Downloading web creator player API JSON\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: [youtube] xqvCmoLULNY: Sign in to confirm you’re not a bot. This helps protect our community. Learn more\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-08-22 08:30:23.552248 Error at video xqvCmoLULNY, skipping\n",
      "2024-08-22 08:30:23.552297 ERROR: [youtube] xqvCmoLULNY: Sign in to confirm you’re not a bot. This helps protect our community. Learn more\n",
      "[youtube] Extracting URL: https://www.youtube.com/watch?v=IB_icWRzi4E\n",
      "[youtube] IB_icWRzi4E: Downloading webpage\n",
      "[youtube] IB_icWRzi4E: Downloading ios player API JSON\n",
      "[youtube] IB_icWRzi4E: Downloading web creator player API JSON\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: [youtube] IB_icWRzi4E: Sign in to confirm you’re not a bot. This helps protect our community. Learn more\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-08-22 08:30:24.139454 Error at video IB_icWRzi4E, skipping\n",
      "2024-08-22 08:30:24.139505 ERROR: [youtube] IB_icWRzi4E: Sign in to confirm you’re not a bot. This helps protect our community. Learn more\n",
      "{'download': {'video_infos': []}}\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'video_id'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m thread \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconfigurable\u001b[39m\u001b[38;5;124m\"\u001b[39m: {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mthread_id\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m1\u001b[39m\u001b[38;5;124m\"\u001b[39m}}\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43ms\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mgraph\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43m{\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtask\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mi wanna teach people how to do squats\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mthread\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mprint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/databuilder_agent/lib/python3.12/site-packages/langgraph/pregel/__init__.py:1029\u001b[0m, in \u001b[0;36mPregel.stream\u001b[0;34m(self, input, config, stream_mode, output_keys, interrupt_before, interrupt_after, debug)\u001b[0m\n\u001b[1;32m   1026\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m   1028\u001b[0m \u001b[38;5;66;03m# panic on failure or timeout\u001b[39;00m\n\u001b[0;32m-> 1029\u001b[0m \u001b[43m_panic_or_proceed\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdone\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minflight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1030\u001b[0m \u001b[38;5;66;03m# don't keep futures around in memory longer than needed\u001b[39;00m\n\u001b[1;32m   1031\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m done, inflight, futures\n",
      "File \u001b[0;32m~/.conda/envs/databuilder_agent/lib/python3.12/site-packages/langgraph/pregel/__init__.py:1456\u001b[0m, in \u001b[0;36m_panic_or_proceed\u001b[0;34m(done, inflight, step, timeout_exc_cls)\u001b[0m\n\u001b[1;32m   1454\u001b[0m             inflight\u001b[38;5;241m.\u001b[39mpop()\u001b[38;5;241m.\u001b[39mcancel()\n\u001b[1;32m   1455\u001b[0m         \u001b[38;5;66;03m# raise the exception\u001b[39;00m\n\u001b[0;32m-> 1456\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m exc\n\u001b[1;32m   1458\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m inflight:\n\u001b[1;32m   1459\u001b[0m     \u001b[38;5;66;03m# if we got here means we timed out\u001b[39;00m\n\u001b[1;32m   1460\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m inflight:\n\u001b[1;32m   1461\u001b[0m         \u001b[38;5;66;03m# cancel all pending tasks\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/databuilder_agent/lib/python3.12/site-packages/langgraph/pregel/executor.py:60\u001b[0m, in \u001b[0;36mBackgroundExecutor.done\u001b[0;34m(self, task)\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdone\u001b[39m(\u001b[38;5;28mself\u001b[39m, task: concurrent\u001b[38;5;241m.\u001b[39mfutures\u001b[38;5;241m.\u001b[39mFuture) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     59\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 60\u001b[0m         \u001b[43mtask\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     61\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m GraphInterrupt:\n\u001b[1;32m     62\u001b[0m         \u001b[38;5;66;03m# This exception is an interruption signal, not an error\u001b[39;00m\n\u001b[1;32m     63\u001b[0m         \u001b[38;5;66;03m# so we don't want to re-raise it on exit\u001b[39;00m\n\u001b[1;32m     64\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtasks\u001b[38;5;241m.\u001b[39mpop(task)\n",
      "File \u001b[0;32m~/.conda/envs/databuilder_agent/lib/python3.12/concurrent/futures/_base.py:449\u001b[0m, in \u001b[0;36mFuture.result\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    447\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m CancelledError()\n\u001b[1;32m    448\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;241m==\u001b[39m FINISHED:\n\u001b[0;32m--> 449\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__get_result\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    451\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_condition\u001b[38;5;241m.\u001b[39mwait(timeout)\n\u001b[1;32m    453\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;129;01min\u001b[39;00m [CANCELLED, CANCELLED_AND_NOTIFIED]:\n",
      "File \u001b[0;32m~/.conda/envs/databuilder_agent/lib/python3.12/concurrent/futures/_base.py:401\u001b[0m, in \u001b[0;36mFuture.__get_result\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    399\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception:\n\u001b[1;32m    400\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 401\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception\n\u001b[1;32m    402\u001b[0m     \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    403\u001b[0m         \u001b[38;5;66;03m# Break a reference cycle with the exception in self._exception\u001b[39;00m\n\u001b[1;32m    404\u001b[0m         \u001b[38;5;28mself\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/databuilder_agent/lib/python3.12/concurrent/futures/thread.py:58\u001b[0m, in \u001b[0;36m_WorkItem.run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     55\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 58\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     59\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[1;32m     60\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfuture\u001b[38;5;241m.\u001b[39mset_exception(exc)\n",
      "File \u001b[0;32m~/.conda/envs/databuilder_agent/lib/python3.12/site-packages/langgraph/pregel/retry.py:25\u001b[0m, in \u001b[0;36mrun_with_retry\u001b[0;34m(task, retry_policy)\u001b[0m\n\u001b[1;32m     23\u001b[0m task\u001b[38;5;241m.\u001b[39mwrites\u001b[38;5;241m.\u001b[39mclear()\n\u001b[1;32m     24\u001b[0m \u001b[38;5;66;03m# run the task\u001b[39;00m\n\u001b[0;32m---> 25\u001b[0m \u001b[43mtask\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mproc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtask\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minput\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtask\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;66;03m# if successful, end\u001b[39;00m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/databuilder_agent/lib/python3.12/site-packages/langchain_core/runnables/base.py:2876\u001b[0m, in \u001b[0;36mRunnableSequence.invoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m   2874\u001b[0m context\u001b[38;5;241m.\u001b[39mrun(_set_config_context, config)\n\u001b[1;32m   2875\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m-> 2876\u001b[0m     \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mcontext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstep\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2877\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   2878\u001b[0m     \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m context\u001b[38;5;241m.\u001b[39mrun(step\u001b[38;5;241m.\u001b[39minvoke, \u001b[38;5;28minput\u001b[39m, config)\n",
      "File \u001b[0;32m~/.conda/envs/databuilder_agent/lib/python3.12/site-packages/langgraph/utils.py:102\u001b[0m, in \u001b[0;36mRunnableCallable.invoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m    100\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m accepts_config(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunc):\n\u001b[1;32m    101\u001b[0m         kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconfig\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m config\n\u001b[0;32m--> 102\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[43mcontext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    103\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(ret, Runnable) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrecurse:\n\u001b[1;32m    104\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m ret\u001b[38;5;241m.\u001b[39minvoke(\u001b[38;5;28minput\u001b[39m, config)\n",
      "Cell \u001b[0;32mIn[10], line 184\u001b[0m, in \u001b[0;36mdetect_segments_node\u001b[0;34m(state)\u001b[0m\n\u001b[1;32m    181\u001b[0m min_segment_seconds \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m3\u001b[39m\n\u001b[1;32m    182\u001b[0m fps \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m25\u001b[39m\n\u001b[0;32m--> 184\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m video_clip_results \u001b[38;5;129;01min\u001b[39;00m \u001b[43mclip_results\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroupby\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mvideo_id\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m:\n\u001b[1;32m    185\u001b[0m     probs \u001b[38;5;241m=\u001b[39m video_clip_results[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprobs\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mvalues\n\u001b[1;32m    186\u001b[0m     probs \u001b[38;5;241m=\u001b[39m smoother\u001b[38;5;241m.\u001b[39msmooth(probs)\u001b[38;5;241m.\u001b[39msmooth_data[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[0;32m~/.conda/envs/databuilder_agent/lib/python3.12/site-packages/pandas/core/frame.py:9183\u001b[0m, in \u001b[0;36mDataFrame.groupby\u001b[0;34m(self, by, axis, level, as_index, sort, group_keys, observed, dropna)\u001b[0m\n\u001b[1;32m   9180\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m level \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m by \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   9181\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou have to supply one of \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mby\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m and \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlevel\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m-> 9183\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mDataFrameGroupBy\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   9184\u001b[0m \u001b[43m    \u001b[49m\u001b[43mobj\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   9185\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkeys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mby\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   9186\u001b[0m \u001b[43m    \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   9187\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlevel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlevel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   9188\u001b[0m \u001b[43m    \u001b[49m\u001b[43mas_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mas_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   9189\u001b[0m \u001b[43m    \u001b[49m\u001b[43msort\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msort\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   9190\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgroup_keys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup_keys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   9191\u001b[0m \u001b[43m    \u001b[49m\u001b[43mobserved\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mobserved\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   9192\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdropna\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdropna\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   9193\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/databuilder_agent/lib/python3.12/site-packages/pandas/core/groupby/groupby.py:1329\u001b[0m, in \u001b[0;36mGroupBy.__init__\u001b[0;34m(self, obj, keys, axis, level, grouper, exclusions, selection, as_index, sort, group_keys, observed, dropna)\u001b[0m\n\u001b[1;32m   1326\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropna \u001b[38;5;241m=\u001b[39m dropna\n\u001b[1;32m   1328\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m grouper \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1329\u001b[0m     grouper, exclusions, obj \u001b[38;5;241m=\u001b[39m \u001b[43mget_grouper\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1330\u001b[0m \u001b[43m        \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1331\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkeys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1332\u001b[0m \u001b[43m        \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1333\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlevel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlevel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1334\u001b[0m \u001b[43m        \u001b[49m\u001b[43msort\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msort\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1335\u001b[0m \u001b[43m        \u001b[49m\u001b[43mobserved\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobserved\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mno_default\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobserved\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1336\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdropna\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdropna\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1337\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1339\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m observed \u001b[38;5;129;01mis\u001b[39;00m lib\u001b[38;5;241m.\u001b[39mno_default:\n\u001b[1;32m   1340\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28many\u001b[39m(ping\u001b[38;5;241m.\u001b[39m_passed_categorical \u001b[38;5;28;01mfor\u001b[39;00m ping \u001b[38;5;129;01min\u001b[39;00m grouper\u001b[38;5;241m.\u001b[39mgroupings):\n",
      "File \u001b[0;32m~/.conda/envs/databuilder_agent/lib/python3.12/site-packages/pandas/core/groupby/grouper.py:1043\u001b[0m, in \u001b[0;36mget_grouper\u001b[0;34m(obj, key, axis, level, sort, observed, validate, dropna)\u001b[0m\n\u001b[1;32m   1041\u001b[0m         in_axis, level, gpr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m, gpr, \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1042\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1043\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(gpr)\n\u001b[1;32m   1044\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(gpr, Grouper) \u001b[38;5;129;01mand\u001b[39;00m gpr\u001b[38;5;241m.\u001b[39mkey \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1045\u001b[0m     \u001b[38;5;66;03m# Add key to exclusions\u001b[39;00m\n\u001b[1;32m   1046\u001b[0m     exclusions\u001b[38;5;241m.\u001b[39madd(gpr\u001b[38;5;241m.\u001b[39mkey)\n",
      "\u001b[0;31mKeyError\u001b[0m: 'video_id'"
     ]
    }
   ],
   "source": [
    "thread = {\"configurable\": {\"thread_id\": \"1\"}}\n",
    "for s in graph.stream(\n",
    "    {\n",
    "        \"task\": \"i wanna teach people how to do squats\",\n",
    "    },\n",
    "    thread,\n",
    "):\n",
    "    print(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "datagen",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
