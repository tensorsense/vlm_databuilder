{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import TypedDict, Annotated, Sequence, List, Optional\n",
    "import operator\n",
    "\n",
    "from langchain_core.messages import BaseMessage, HumanMessage, SystemMessage\n",
    "from langchain.pydantic_v1 import BaseModel, Field"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import AzureChatOpenAI\n",
    "\n",
    "llm = AzureChatOpenAI(\n",
    "    temperature=0.0,\n",
    "    azure_deployment=\"gpt4o\",\n",
    "    openai_api_version=\"2023-07-01-preview\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VideoInfo(BaseModel):\n",
    "    video_id: str\n",
    "    url: str\n",
    "    relative_video_path: str\n",
    "    subs: str\n",
    "    transcript: str\n",
    "\n",
    "\n",
    "class SegmentInfo(BaseModel):  # , Generic[OutputSchema]):\n",
    "    start_timestamp: str\n",
    "    end_timestamp: str\n",
    "    fps: float\n",
    "    # segment_info: Optional[OutputSchema]\n",
    "    video_id: str\n",
    "    # _frames: Optional[\n",
    "    # list[np.array]\n",
    "    # ]  # List of raw frames that got into LLM. Added for debugging purposes.\n",
    "\n",
    "    # @classmethod\n",
    "    # def from_frames(cls, start_frame, end_frame, fps, **kwargs):\n",
    "    #     return cls(\n",
    "    #         start_timestamp=seconds_to_ts(start_frame / fps),\n",
    "    #         end_timestamp=seconds_to_ts(end_frame / fps),\n",
    "    #         fps=fps,\n",
    "    #         **kwargs,\n",
    "    #     )\n",
    "\n",
    "    @classmethod\n",
    "    def from_seconds(cls, start_seconds, end_seconds, **kwargs):\n",
    "        return cls(\n",
    "            start_timestamp=seconds_to_ts(start_seconds),\n",
    "            end_timestamp=seconds_to_ts(end_seconds),\n",
    "            **kwargs,\n",
    "        )\n",
    "\n",
    "    # def to_str(self, skip: list[str] = []):\n",
    "    #     # skip -> fields from segment_info\n",
    "    #     # dict() works both with pydantic model and with with unparsed dict\n",
    "    #     if self.segment_info:\n",
    "    #         d = dict(self.segment_info)\n",
    "    #         for s in skip:\n",
    "    #             del d[s]\n",
    "    #         d = \": \" + json.dumps(d)\n",
    "    #     else:\n",
    "    #         d = \"\"\n",
    "    #     return f\"{self.start_timestamp}-{self.end_timestamp}{d}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Create the state\n",
    "\n",
    "class AgentState(TypedDict):\n",
    "\tsearch_queries: List[str]\n",
    "\tvideo_ids: List[str]\n",
    "\tvideo_infos: List[VideoInfo]\n",
    "\tclip_text_prompts = List[str]\n",
    "\tsegment_infos: List[SegmentInfo]\n",
    "\tclues = List[str]\n",
    "\tannotations = List[str]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Set prompts\n",
    "\n",
    "GEN_QUERIES_PROMPT = (\n",
    "    \"You a helping the user to find a very large and diverse set of videos on a video hosting service.\",\n",
    "    \"A user will only describe which videos they are looking for and how many queries they need.\",\n",
    ")\n",
    "\n",
    "# prompt='I want to find instructional videos about how to do squats.',\n",
    "# num_queries_prompt = f'I need {num_queries} queries'\n",
    "\n",
    "EXTRACT_CLUES_PROMPT = \"\"\"You are a highly intelligent data investigator.  \n",
    "You take unstructured damaged data and look for clues that could help restore the initial information\n",
    "and extract important insights from it.\n",
    "You are the best one for this job in the world because you are a former detective. \n",
    "You care about even the smallest details, and your guesses about what happened in the initial file\n",
    "even at very limited inputs are usually absolutely right.  \n",
    "You use deductive and inductive reasoning at the highest possible quality.\n",
    "\n",
    "#YOUR TODAY'S JOB\n",
    "The user needs to learn about what happens in a specific segment of a video file. Your job is to help the user by providing clues that would help the user make the right assumption.\n",
    "The user will provide you with: \n",
    "1. Instructions about what kind of information the user is trying to obtain.\n",
    "2. A list of time codes of the segments in format \"<HH:MM:SS.ms>-<HH:MM:SS.ms>\". All the provided segment of the video contain what the user is looking for, but other parts of the video might have different content.\n",
    "3. A transcript of the *full video* in format of \"<HH.MM.SS>\\\\n<text>\"\n",
    "\n",
    "Your task:\n",
    "1. Read the transcript.\n",
    "2. Provide the clues in a given format.\n",
    "3. Provied any other info requested by the user.\n",
    "\n",
    "#RULES\n",
    "!!! VERY IMPORTANT !!!\n",
    "1. Rely only on the data provided in the transcript. Do not improvise. All the quotes and corresponding timestamps must be taken from the transcript. Quote timestamps must be taken directly from the transcript.\n",
    "2. Your job is to find the data already provided in the transcript.\n",
    "3. Analyze every segment. Only skip a segment if there is no information about it in the trascript.\n",
    "4. For local clues, make sure that the quotes that you provide are located inside the segment. To do this, double check the timestamps from the transcript and the segment.\n",
    "5. For all clues, make sure that the quotes exactly correspond to the timestamps that you provide.\n",
    "6. When making clues, try as much as possible to make them describe specifically what is shown in the segment.\n",
    "7. Follow the format output.\n",
    "8. Be very careful with details. Don't generalize. Always double check your results.\n",
    "\n",
    "Please, help the user find relevant clues to reconstruct the information they are looking for, for each provided segment.\n",
    "\n",
    "WHAT IS A CLUE: A *clue*, in the context of reconstructing narratives from damaged data, \n",
    "is a fragment of information extracted from a corrupted or incomplete source that provides \n",
    "insight into the original content. These fragments serve as starting points for inference \n",
    "and deduction, allowing researchers to hypothesize about the fuller context or meaning of \n",
    "the degraded material. The process of identifying and interpreting clues involves both objective analysis of the \n",
    "available data and subjective extrapolation based on domain knowledge, contextual understanding, \n",
    "and logical reasoning.\n",
    "\n",
    "Here is what the user expects to have from you:\n",
    "1. *Local clues* that would help the user undestand how the thing they are looking for happens inside the segment. Local clues for a segment are generated from quotes inside a specific segment.\n",
    "2. *Global clues* that would help the user understand how the thing they are looking for happens inside the segment. Global clues for a segment are generated from quotes all around the video, but are very relevant to the specific that they are provided for.\n",
    "3. *Logical inferences* that could help the user understand how the thing they are looking for happens inside the segment. Logical inferences for a segment are deducted from local and global clues for this segment.\n",
    "\n",
    "!!!IT IS EXTREMELY IMPORTANT TO DELIVER ALL THREE THINGS!!!\n",
    "\"\"\"\n",
    "\n",
    "# also MANY a structured output prompt\n",
    "\n",
    "# EXTRACT_CLUES_PROMPT = \"\"\"\n",
    "# \"User's instructions: The provided video is a tutorial about how to perform squats.\n",
    "\n",
    "# I need to understand HOW THE PERSON SHOWN IN EACH SEGMENT PERFORMS SQUATS IN THIS SEGMENT.\n",
    "# What is done correctly.\n",
    "# What mistakes they make. Why these mistakes happen.\n",
    "# How these mistakes could be improved.\n",
    "\n",
    "# It is very improtant that the information that you provide would describe how the person shown in the segment is doing squats, and not some generic advice that is unrelated to the visual information.\n",
    "# \"\"\"\n",
    "\n",
    "# prompt.append('Segment timecodes and optional additional information:\\n' + '\\n'.join([s.to_str(skip=[filter_by] if filter_by else []) for s in video_segments_part]))\n",
    "# prompt.append('Transcript:\\n' + transcript)\n",
    "\n",
    "\n",
    "GEN_ANNOTATIONS_PROMPT = \"\"\"You are a helpful assistant that performs high quality data investigation and transformation.\n",
    "                You will be given a JSON object with clues and other helpful information about what's going on \n",
    "                in a specific part of a video file. This part is called a segment. Your job is to:\n",
    "                1. Read this JSON object carefully\n",
    "                2. Answer user's questions about this segment\n",
    "                3. Provide the answer as a JSON object in a schema provided by the user\n",
    "                Important rules:\n",
    "                1. You can only rely on data presented in a provided JSON object. Don't improvise.\n",
    "                2. Follow user's request carefully.\n",
    "                3. Don't rush to deliver the answer. Take some time to think. Make a deep breath. Then start writing.\n",
    "                4. If you want to output field as empty (null), output it as JSON null (without quotes), not as a string \"null\". \n",
    "\"\"\"\n",
    "\n",
    "\n",
    "# human_prompt = \"\"\"\n",
    "# You are given a JSON object that contains clues about segments of a video with timecodes.\n",
    "# !!!! For each segment provided in a JSON object you need to answer on the following questions:\n",
    "# 1. Given the data found in the JSON object, what is a probability that this part contains a footage of a person doing squats? [the answer could be only \"high\", \"medium\", \"low\", or null (if impossible to infer from the provided data)]\n",
    "# 2. Given the data found in the JSON object and even if the answer on the previous question is \"low\", does this person do squats right, wrong, or mixed? [the answer could be only \"right\", \"wrong\", \"mixed\", or null (if impossible to infer from the provided data)]\n",
    "# 3. Given the data found in the JSON object, what exactly does thing person do right and/or wrong regarding their squats technique? [the answer should be clear and focused on body parts]\n",
    "# 4. If the answer on the previous question contains description of wrong technique, explain how to fix these mistakes using your \"own knowledge\" like you are a sports coach.\n",
    "# \"\"\"\n",
    "\n",
    "# for clue in clues_part:\n",
    "#     prompt.append(\"Segment:\\n\" + json.dumps(clue))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datagen import DatagenConfig, get_video_ids, download_videos, detect_segments_clip, generate_clues, generate_annotations\n",
    "\n",
    "config_params = {\n",
    "    \"openai\": {\n",
    "        \"type\": \"azure\",  # openai/azure\n",
    "        \"temperature\": \"1\",\n",
    "        \"deployment\": \"gpt4o\",  # model for openai / deployment for azure\n",
    "    },\n",
    "    \"data_dir\": \"./tmp/squats\",\n",
    "}\n",
    "\n",
    "!mkdir -p {config_params[\"data_dir\"]}\n",
    "\n",
    "# this config handles all the bookeeping so you need to pass it everywhere.\n",
    "config = DatagenConfig(**config_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scrapetube\n",
    "import yt_dlp\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "from collections import defaultdict\n",
    "from datagen.core.sub_utils import vtt_to_txt\n",
    "from datagen.detect_segments import get_segments\n",
    "import torch\n",
    "from transformers import AutoModel, AutoProcessor\n",
    "import pandas as pd\n",
    "from tsmoothie.smoother import LowessSmoother"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import decord\n",
    "import math\n",
    "import numpy as np\n",
    "\n",
    "decord.bridge.set_bridge(\"torch\")\n",
    "\n",
    "\n",
    "class VideoInferenceDataset(torch.utils.data.IterableDataset):\n",
    "    def __init__(self, video_infos: List[VideoInfo]):\n",
    "        super(VideoInferenceDataset).__init__()\n",
    "\n",
    "        self.video_infos = video_infos\n",
    "        self.frame_generator = self.get_frame_generator(video_infos)\n",
    "\n",
    "    @staticmethod\n",
    "    def get_frame_generator(video_infos):\n",
    "\n",
    "        for video_info in video_infos:\n",
    "            video_path = Path(video_info.relative_video_path)\n",
    "            vr = decord.VideoReader(str(video_path))\n",
    "            num_frames = len(vr)\n",
    "            fps = math.ceil(num_frames / video_info.duration)\n",
    "            frame_indices = range(0, num_frames, fps)\n",
    "\n",
    "            for frame_idx in frame_indices:\n",
    "                frame = vr[frame_idx].asnumpy()\n",
    "                yield {\n",
    "                    \"frame\": frame,\n",
    "                    \"frame_idx\": frame_idx,\n",
    "                    \"video_id\": video_info.video_id,\n",
    "                }\n",
    "\n",
    "    def __next__(self):\n",
    "        return next(self.frame_generator)\n",
    "\n",
    "    def __iter__(self):\n",
    "        # worker_info = torch.utils.data.get_worker_info()\n",
    "        return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Create nodes\n",
    "\n",
    "\n",
    "def gen_queries_node(state: AgentState):\n",
    "    class QueryList(BaseModel):\n",
    "        \"\"\"A list of queries to find videos on a video hosting service\"\"\"\n",
    "\n",
    "        search_queries: list[str] = Field(default=None, description=\"a list of queries\")\n",
    "\n",
    "    messages = [\n",
    "        SystemMessage(content=GEN_QUERIES_PROMPT),\n",
    "        HumanMessage(content=state[\"task\"]),\n",
    "    ]\n",
    "\n",
    "    model = llm.with_structured_output(QueryList)\n",
    "    response: QueryList = model.invoke(messages)\n",
    "\n",
    "    return {\"search_queries\": response.search_queries}\n",
    "\n",
    "\n",
    "def get_video_ids_node(state: AgentState):\n",
    "\n",
    "    queries = state[\"search_queries\"]\n",
    "    videos_per_query = 2\n",
    "    sleep = 0\n",
    "    sort_by = \"relevance\"\n",
    "    results_type = \"video\"\n",
    "    only_creative_commons = False\n",
    "\n",
    "    video_ids = set()\n",
    "    for query in queries:\n",
    "        for video in scrapetube.get_search(\n",
    "            query=query,\n",
    "            limit=videos_per_query,\n",
    "            sleep=sleep,\n",
    "            sort_by=sort_by,\n",
    "            results_type=results_type,\n",
    "        ):\n",
    "            video_ids.add(video[\"videoId\"])\n",
    "    video_ids = list(video_ids)\n",
    "\n",
    "    if only_creative_commons:\n",
    "        video_ids_cc = []\n",
    "        for i in video_ids:\n",
    "            YDL_OPTIONS = {\n",
    "                \"quiet\": True,\n",
    "                \"simulate\": True,\n",
    "                \"forceurl\": True,\n",
    "            }\n",
    "            with yt_dlp.YoutubeDL(YDL_OPTIONS) as ydl:\n",
    "                info = ydl.extract_info(f\"youtube.com/watch?v={i}\", download=False)\n",
    "            if \"creative commons\" in info.get(\"license\", \"\").lower():\n",
    "                video_ids_cc.append(i)\n",
    "        video_ids = video_ids_cc\n",
    "\n",
    "    return {\"video_ids\": video_ids}\n",
    "\n",
    "\n",
    "def download_node(state: AgentState):\n",
    "\n",
    "    LOCAL_ROOT = Path(\"./tmp/agent_squats\").resolve()\n",
    "    video_dir = LOCAL_ROOT / \"videos\"\n",
    "    sub_dir = LOCAL_ROOT / \"subs\"\n",
    "\n",
    "    discard_path = LOCAL_ROOT / \"videos_without_subs\"\n",
    "    discard_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    video_ids = state[\"video_ids\"]\n",
    "\n",
    "    downloaded_video_ids = [video_path.name for video_path in video_dir.glob(\"*.mp4\")]\n",
    "    downloaded_video_ids += [\n",
    "        video_path.name for video_path in discard_path.glob(\"*.mp4\")\n",
    "    ]\n",
    "\n",
    "    only_with_transcripts = True\n",
    "\n",
    "    YDL_OPTIONS = {\n",
    "        \"writeautomaticsub\": True,\n",
    "        \"subtitleslangs\": [\"en\"],\n",
    "        \"subtitlesformat\": \"vtt\",\n",
    "        \"overwrites\": False,\n",
    "        \"format\": \"mp4\",\n",
    "        \"outtmpl\": {\n",
    "            \"default\": video_dir.as_posix() + \"/%(id)s.%(ext)s\",\n",
    "            \"subtitle\": sub_dir.as_posix() + \"/%(id)s\",\n",
    "        },\n",
    "    }\n",
    "\n",
    "    video_infos = []\n",
    "\n",
    "    with yt_dlp.YoutubeDL(YDL_OPTIONS) as ydl:\n",
    "        for video_id in video_ids:\n",
    "            if video_id not in downloaded_video_ids:\n",
    "                try:\n",
    "                    url = f\"https://www.youtube.com/watch?v={video_id}\"\n",
    "                    ydl.download(url)\n",
    "                except Exception as e:\n",
    "                    print(datetime.now(), f\"Error at video {video_id}, skipping\")\n",
    "                    print(datetime.now(), e)\n",
    "                    continue\n",
    "\n",
    "            video_path = Path(ydl.prepare_filename({\"id\": video_id}))\n",
    "            sub_path = Path(ydl.prepare_filename({\"id\": video_id, \"ext\": \"en.vtt\"}))\n",
    "\n",
    "            with sub_path.open(\"r\") as f:\n",
    "                subs = f.read()\n",
    "\n",
    "            transcript = vtt_to_txt(sub_path)\n",
    "\n",
    "            video_info = VideoInfo.from_local_download(\n",
    "                video_id=video_id,\n",
    "                url=url,\n",
    "                video_path=video_path.relative_to(LOCAL_ROOT).as_posix(),\n",
    "                subs=subs,\n",
    "                transcript=transcript,\n",
    "            )\n",
    "\n",
    "            video_infos.append(video_info)\n",
    "\n",
    "    if only_with_transcripts:\n",
    "        filtered_video_infos = []\n",
    "        for video_info in video_infos:\n",
    "            if video_info.transcript:\n",
    "                filtered_video_infos.append(video_info)\n",
    "            else:\n",
    "                video_path = LOCAL_ROOT / video_info.video_path\n",
    "                video_path.rename(discard_path / video_path.name)\n",
    "        video_infos = filtered_video_infos\n",
    "\n",
    "    return {\"video_infos\": video_infos}\n",
    "\n",
    "\n",
    "def detect_segments_node(state: AgentState):\n",
    "\n",
    "    clip_text_prompts = state[\"clip_text_prompts\"]\n",
    "    video_infos = state[\"video_infos\"]\n",
    "\n",
    "    CLIP_MODEL_ID = \"google/siglip-so400m-patch14-384\"\n",
    "\n",
    "    model = AutoModel.from_pretrained(CLIP_MODEL_ID)\n",
    "    processor = AutoProcessor.from_pretrained(CLIP_MODEL_ID)\n",
    "\n",
    "    dataset = VideoInferenceDataset(video_infos)\n",
    "    dataloader = torch.utils.data.DataLoader(dataset, num_workers=0, batch_size=100)\n",
    "\n",
    "    smoother = LowessSmoother(smooth_fraction=0.02, iterations=1)\n",
    "\n",
    "    clip_results_dict = defaultdict(list)\n",
    "\n",
    "    while True:\n",
    "        try:\n",
    "            batch = next(iter(dataloader))\n",
    "        except StopIteration:\n",
    "            break\n",
    "\n",
    "        frames = batch[\"frame\"].to(\"cuda\")\n",
    "\n",
    "        inputs = processor(\n",
    "            images=frames,\n",
    "            text=clip_text_prompts,\n",
    "            return_tensors=\"pt\",\n",
    "            padding=True,\n",
    "            truncation=True,\n",
    "        )\n",
    "\n",
    "        logits = model(**inputs)\n",
    "        probs = torch.nn.functional.sigmoid(logits)\n",
    "\n",
    "        for sample, prob in zip(batch, probs):\n",
    "            video_id = sample[\"video_id\"]\n",
    "            frame_idx = sample[\"frame_idx\"]\n",
    "            clip_results_dict[\"video_id\"].append(video_id)\n",
    "            clip_results_dict[\"frame_idx\"].append(frame_idx)\n",
    "            clip_results_dict[\"probs\"].append(prob.item())\n",
    "\n",
    "    clip_results = pd.DataFrame(clip_results_dict)\n",
    "\n",
    "    max_gap_seconds = 1\n",
    "    fps_sampling = 1\n",
    "    min_prob = 0.1\n",
    "    min_segment_seconds = 3\n",
    "    fps = 25\n",
    "\n",
    "    for video_clip_results in clip_results.groupby(\"video_id\"):\n",
    "        probs = video_clip_results[\"probs\"].values\n",
    "        probs = smoother.smooth(probs).smooth_data[0]\n",
    "        segments_start_end = get_segments(\n",
    "            probs,\n",
    "            max_gap=round(max_gap_seconds * fps_sampling),\n",
    "            min_prob=min_prob,\n",
    "            min_segment=round(min_segment_seconds * fps_sampling),\n",
    "        )\n",
    "        segments = []\n",
    "        for start, end in segments_start_end:\n",
    "            segments.append(\n",
    "                SegmentInfo.from_seconds(\n",
    "                    start,\n",
    "                    end,\n",
    "                    fps=fps,\n",
    "                    video_id=\"a\",\n",
    "                )\n",
    "            )\n",
    "\n",
    "    return {\"segments\": segments}\n",
    "\n",
    "\n",
    "# def extract_clues_node(state: AgentState):\n",
    "#     clues = []\n",
    "\n",
    "#     clues = generate_clues(\n",
    "#         # video_ids=['byxWus7BwfQ'],\n",
    "#         config=config,\n",
    "#         human_prompt=human_prompt,\n",
    "#         segments_per_call=5,  # the output might be quite long, so need to limit number of segments per gpt call to respect max output legnth\n",
    "#         raise_on_error=True,  # interrupt when encountering an error. Useful for debugging.\n",
    "#     )\n",
    "\n",
    "#     return {\"clues\": clues}\n",
    "\n",
    "\n",
    "# def gen_annotations_node(state: AgentState):\n",
    "\n",
    "#     class SegmentFeedback(BaseModel):\n",
    "#         \"\"\"\n",
    "#         —> GOOD EXAMPLES:\n",
    "#             \"wrong\":\"Knees caving in: This can stress the knees and reduce effectiveness\"\n",
    "#             \"correction\":\"Focus on keeping knees aligned with your toes.\"\n",
    "#             \"wrong\":\"Rounding the back: This increases the risk of back injuries\"\n",
    "#             \"correction\":\"Keep your chest up and maintain a neutral spine throughout the movement.\"\n",
    "#             \"wrong\":\"Heels are lifting off the ground: this shifts the weight forward, reducing stability\"\n",
    "#             \"correction\":\" Keep your weight on your heels and press through them as you rise.\"\n",
    "#             \"right\":\"Chest and shoulders: The chest is up, and the shoulders are back, maintaining an upright torso.\"\n",
    "#             \"correction\":null\n",
    "#         —> BAD EXAMPLES:\n",
    "#             \"wrong\":\"knees\"\n",
    "#             \"correction\":\"fix knees\"\n",
    "#             \"wrong\":\"back looks funny\"\n",
    "#             \"correction\":\"make back better\"\n",
    "#             \"wrong\":\"feet are doing something\"\n",
    "#             \"correction\":\"feet should be different\"\n",
    "#             \"right\":\"arms\"\n",
    "#             \"correction\":\"arms are fine i think\"\n",
    "#         —> BAD EXAMPLES END HERE\n",
    "#         \"\"\"\n",
    "\n",
    "#         right: Optional[str] = Field(description=\"what was right in the performance\")\n",
    "#         wrong: Optional[str] = Field(description=\"what was wrong in the performance\")\n",
    "#         correction: Optional[str] = Field(\n",
    "#             description=\"how and in what ways it the performance could be improved\"\n",
    "#         )\n",
    "\n",
    "#     # The segment timestamps are taken from the provided information.\n",
    "#     class SegmentAnnotation(BaseModel):\n",
    "#         squats_probability: Optional[str] = Field(\n",
    "#             description=\"how high is the probability that the person is doing squats in the segment: low, medium, high, unknown(null)\"\n",
    "#         )\n",
    "#         squats_technique_correctness: Optional[str] = Field(\n",
    "#             description=\"correctness of the squat technique.\"\n",
    "#         )\n",
    "#         squats_feedback: Optional[SegmentFeedback] = Field(\n",
    "#             description=\"what was right and wrong in the squat perfomance in the segment. When the technique is incorrect, provide instructions how to correct them.\"\n",
    "#         )\n",
    "\n",
    "#     annotations = generate_annotations(\n",
    "#         human_prompt=human_prompt,\n",
    "#         config=config,\n",
    "#         segments_per_call=5,\n",
    "#         annotation_schema=SegmentAnnotation,\n",
    "#     )\n",
    "\n",
    "#     return {\"annotations\": annotations}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import StateGraph, END\n",
    "from typing import TypedDict, Annotated, List\n",
    "import operator\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "\n",
    "from langchain_core.messages import AnyMessage, SystemMessage, HumanMessage, AIMessage, ChatMessage\n",
    "\n",
    "memory = MemorySaver()\n",
    "# memory = SqliteSaver.from_conn_string(\":memory:\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "builder = StateGraph(AgentState)\n",
    "\n",
    "builder.add_node(\"generate_queries\", gen_queries_node)\n",
    "builder.add_node(\"get_video_ids\", get_video_ids_node)\n",
    "builder.add_node(\"download\", download_node)\n",
    "builder.add_node(\"detect_segments\", detect_segments_node)\n",
    "# builder.add_node(\"extract_clues\", extract_clues_node)\n",
    "# builder.add_node(\"gen_annotations\", gen_annotations_node)\n",
    "\n",
    "builder.set_entry_point(\"generate_queries\")\n",
    "\n",
    "# builder.add_conditional_edges(\n",
    "#     \"generate\", \n",
    "#     should_continue, \n",
    "#     {END: END, \"reflect\": \"reflect\"}\n",
    "# )\n",
    "\n",
    "builder.add_edge(\"generate_queries\", \"get_video_ids\")\n",
    "builder.add_edge(\"get_video_ids\", \"download\")\n",
    "builder.add_edge(\"download\", \"detect_segments\")\n",
    "builder.add_edge(\"detect_segments\", END)\n",
    "\n",
    "# builder.add_edge(\"detect_segments\", \"extract_clues\")\n",
    "# builder.add_edge(\"extract_clues\", \"gen_annotations\")\n",
    "# builder.add_edge(\"gen_annotations\", END)\n",
    "\n",
    "graph = builder.compile(checkpointer=memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thread = {\"configurable\": {\"thread_id\": \"1\"}}\n",
    "for s in graph.stream(\n",
    "    {\n",
    "        \"task\": \"what is the difference between langchain and langsmith\",\n",
    "    },\n",
    "    thread,\n",
    "):\n",
    "    print(s)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "datagen",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
