{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "_ = load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import TypedDict, Annotated, Sequence, List, Optional\n",
    "import operator\n",
    "\n",
    "from langchain_core.messages import BaseMessage, HumanMessage, SystemMessage\n",
    "from langchain.pydantic_v1 import BaseModel, Field\n",
    "from langchain_core.prompts import ChatPromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import AzureChatOpenAI\n",
    "\n",
    "llm = AzureChatOpenAI(\n",
    "    temperature=0.0,\n",
    "    azure_deployment=\"gpt4o\",\n",
    "    openai_api_version=\"2023-07-01-preview\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VideoInfo(BaseModel):\n",
    "    video_id: str\n",
    "    url: str\n",
    "    relative_video_path: str\n",
    "    subs: str\n",
    "    transcript: str\n",
    "\n",
    "\n",
    "class SegmentInfo(BaseModel):\n",
    "    start_timestamp: str\n",
    "    end_timestamp: str\n",
    "    fps: float\n",
    "    video_id: str\n",
    "\n",
    "\n",
    "class LocalClue(BaseModel):\n",
    "    \"\"\"Local clues for a segment\"\"\"\n",
    "\n",
    "    id: str = Field(description=\"LC1,LC2...\")\n",
    "    quote: str = Field(\n",
    "        description=\"the quote from the transcript that was used to create this clue.\"\n",
    "    )\n",
    "    quote_timestamp_start: str = Field(\n",
    "        description=\"the exact start timestamp of the quote.\"\n",
    "    )\n",
    "    quote_timestamp_end: str = Field(\n",
    "        description=\"the exact end timestamp of the quote.\"\n",
    "    )\n",
    "    clue: str = Field(description=\"the main clue data\")\n",
    "\n",
    "\n",
    "class GlobalClue(BaseModel):\n",
    "    \"\"\"Global clues for a segment\"\"\"\n",
    "\n",
    "    id: str = Field(description=\"GC1,GC2...\")\n",
    "    quote: str = Field(\n",
    "        description=\"the quote from the transcript that was used to create this clue.\"\n",
    "    )\n",
    "    quote_timestamp_start: str = Field(\n",
    "        description=\"the exact start timestamp of the quote.\"\n",
    "    )\n",
    "    quote_timestamp_end: str = Field(\n",
    "        description=\"the exact end timestamp of the quote.\"\n",
    "    )\n",
    "    clue: str = Field(description=\"the main clue data.\")\n",
    "    relevance_to_segment: str = Field(\n",
    "        description=\"why do you think this global clue is relevant to the segment you are working with right now.\"\n",
    "    )\n",
    "\n",
    "\n",
    "class LogicalInference(BaseModel):\n",
    "    \"\"\"Logical inferences for a segment\"\"\"\n",
    "\n",
    "    id: str = Field(description=\"LI1,LI2,...\")\n",
    "    description: str = Field(description=\"A concise form of the logical inference.\")\n",
    "    details: str = Field(\n",
    "        description=\"A verbose explanation of what insight about what happens in this segment should be made based on the clues that you found.\"\n",
    "    )\n",
    "\n",
    "\n",
    "class SegmentAnnotation(BaseModel):\n",
    "    local_clues: list[LocalClue] = Field(\n",
    "        description=\"Local clues are inside the segment in terms of timestamps.\"\n",
    "    )\n",
    "    global_clues: list[GlobalClue] = Field(\n",
    "        description=\"Global clues are scattered across the entire transcript.\"\n",
    "    )\n",
    "    logical_inferences: list[LogicalInference] = Field(\n",
    "        description=\"What can we infer about the topic, that the user is looking for in the video, can we make based on the clues inside this segment\"\n",
    "    )\n",
    "\n",
    "\n",
    "class SegmentWithClueInfo(BaseModel):\n",
    "    \"\"\"\n",
    "    Annotation for a video segment.\n",
    "    \"\"\"\n",
    "\n",
    "    start_timestamp: str = Field(\n",
    "        description=\"start timestamp of the segment in format HH:MM:SS.MS\"\n",
    "    )\n",
    "    end_timestamp: str = Field(\n",
    "        description=\"start timestamp of the segment in format HH:MM:SS.MS\"\n",
    "    )\n",
    "    segment_annotation: SegmentAnnotation = Field(\n",
    "        description=\"list of annotations for the segment\"\n",
    "    )\n",
    "\n",
    "\n",
    "class VideoAnnotation(BaseModel):\n",
    "    \"\"\"\n",
    "    Segments of a video.\n",
    "    \"\"\"\n",
    "\n",
    "    segments: list[SegmentWithClueInfo] = Field(\n",
    "        description=\"information about each segment\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Create the state\n",
    "\n",
    "\n",
    "class AgentState(TypedDict):\n",
    "    task: str\n",
    "    search_queries: List[str]\n",
    "    video_ids: List[str]\n",
    "    video_infos: List[VideoInfo]\n",
    "    clip_text_prompts: List[str]\n",
    "    segment_infos: List[SegmentInfo]\n",
    "    clues: List[str]\n",
    "    annotations: List[str]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Set prompts\n",
    "\n",
    "GEN_QUERIES_PROMPT = (\n",
    "    \"You a helping the user to find a very large and diverse set of videos on a video hosting service.\",\n",
    "    \"A user will only describe which videos they are looking for and how many queries they need.\",\n",
    ")\n",
    "\n",
    "# prompt='I want to find instructional videos about how to do squats.',\n",
    "# num_queries_prompt = f'I need {num_queries} queries'\n",
    "\n",
    "EXTRACT_CLUES_PROMPT = \"\"\"You are a highly intelligent data investigator.  \n",
    "You take unstructured damaged data and look for clues that could help restore the initial information\n",
    "and extract important insights from it.\n",
    "You are the best one for this job in the world because you are a former detective. \n",
    "You care about even the smallest details, and your guesses about what happened in the initial file\n",
    "even at very limited inputs are usually absolutely right.  \n",
    "You use deductive and inductive reasoning at the highest possible quality.\n",
    "\n",
    "#YOUR TODAY'S JOB\n",
    "The user needs to learn about what happens in a specific segment of a video file. Your job is to help the user by providing clues that would help the user make the right assumption.\n",
    "The user will provide you with: \n",
    "1. Instructions about what kind of information the user is trying to obtain.\n",
    "2. A list of time codes of the segments in format \"<HH:MM:SS.ms>-<HH:MM:SS.ms>\". All the provided segment of the video contain what the user is looking for, but other parts of the video might have different content.\n",
    "3. A transcript of the *full video* in format of \"<HH.MM.SS>\\\\n<text>\"\n",
    "\n",
    "Your task:\n",
    "1. Read the transcript.\n",
    "2. Provide the clues in a given format.\n",
    "3. Provied any other info requested by the user.\n",
    "\n",
    "#RULES\n",
    "!!! VERY IMPORTANT !!!\n",
    "1. Rely only on the data provided in the transcript. Do not improvise. All the quotes and corresponding timestamps must be taken from the transcript. Quote timestamps must be taken directly from the transcript.\n",
    "2. Your job is to find the data already provided in the transcript.\n",
    "3. Analyze every segment. Only skip a segment if there is no information about it in the trascript.\n",
    "4. For local clues, make sure that the quotes that you provide are located inside the segment. To do this, double check the timestamps from the transcript and the segment.\n",
    "5. For all clues, make sure that the quotes exactly correspond to the timestamps that you provide.\n",
    "6. When making clues, try as much as possible to make them describe specifically what is shown in the segment.\n",
    "7. Follow the format output.\n",
    "8. Be very careful with details. Don't generalize. Always double check your results.\n",
    "\n",
    "Please, help the user find relevant clues to reconstruct the information they are looking for, for each provided segment.\n",
    "\n",
    "WHAT IS A CLUE: A *clue*, in the context of reconstructing narratives from damaged data, \n",
    "is a fragment of information extracted from a corrupted or incomplete source that provides \n",
    "insight into the original content. These fragments serve as starting points for inference \n",
    "and deduction, allowing researchers to hypothesize about the fuller context or meaning of \n",
    "the degraded material. The process of identifying and interpreting clues involves both objective analysis of the \n",
    "available data and subjective extrapolation based on domain knowledge, contextual understanding, \n",
    "and logical reasoning.\n",
    "\n",
    "Here is what the user expects to have from you:\n",
    "1. *Local clues* that would help the user undestand how the thing they are looking for happens inside the segment. Local clues for a segment are generated from quotes inside a specific segment.\n",
    "2. *Global clues* that would help the user understand how the thing they are looking for happens inside the segment. Global clues for a segment are generated from quotes all around the video, but are very relevant to the specific that they are provided for.\n",
    "3. *Logical inferences* that could help the user understand how the thing they are looking for happens inside the segment. Logical inferences for a segment are deducted from local and global clues for this segment.\n",
    "\n",
    "!!!IT IS EXTREMELY IMPORTANT TO DELIVER ALL THREE THINGS!!!\n",
    "\n",
    "        Good local clues examples: [\n",
    "      {\n",
    "        \"id\": \"LC1\",\n",
    "        \"timestamp\": \"00:00:19\",\n",
    "        \"quote\": \"exercises do them wrong and instead of\",\n",
    "        \"clue\": \"This phrase introduces the concept of incorrect exercise form, setting the stage for a demonstration of improper technique.\"\n",
    "      },\n",
    "      {\n",
    "        \"id\": \"LC2\",\n",
    "        \"timestamp\": \"00:00:21\",\n",
    "        \"quote\": \"growing nice quads and glutes you'll\",\n",
    "        \"clue\": \"Mentions the expected benefits of proper squats (muscle growth), implying that these benefits won't be achieved with incorrect form.\"\n",
    "      },\n",
    "      {\n",
    "        \"id\": \"LC3\",\n",
    "        \"timestamp\": \"00:00:22\",\n",
    "        \"quote\": \"feel aches and pains in your knees your\",\n",
    "        \"clue\": \"Directly states negative consequences of improper form, strongly suggesting that this segment demonstrates incorrect technique.\"\n",
    "      },\n",
    "      {\n",
    "        \"id\": \"LC4\",\n",
    "        \"timestamp\": \"00:00:24\",\n",
    "        \"quote\": \"lower back and even your shoulders\",\n",
    "        \"clue\": \"Continuation of LC3, emphasizing multiple areas of potential pain from improper form.\"\n",
    "      },\n",
    "      {\n",
    "        \"id\": \"LC5\",\n",
    "        \"timestamp\": \"00:00:26\",\n",
    "        \"quote\": \"let's see how to do it correctly\",\n",
    "        \"clue\": \"This phrase suggests a transition is about to occur. The incorrect form has been shown, and correct form will follow.\"\n",
    "      }\n",
    "    ]\n",
    "\n",
    "    Double check that the timestamp and the quote that you provide exactly correspond to what you found in the transcript.\n",
    "    For example, if the transcript says:\n",
    "    \"00:05:02\n",
    "    he took the glasses\n",
    "    00:05:04\n",
    "    and gave them to me\"\n",
    "    Then a GOOD output will be:\n",
    "    - timestamp: 00:05:03\n",
    "    - quote: \"he took the glasses and gave them to me\"\n",
    "    And a BAD output would be:\n",
    "    - timestamp: 00:04:02\n",
    "    - quote: \"he gave me the glasses\"\n",
    "\n",
    "    Good global clues examples: [\n",
    "      {\n",
    "        \"id\": \"GC1\",\n",
    "        \"timestamp\": \"00:01:15\",\n",
    "        \"quote\": \"Before we dive into specific techniques, let's talk about safety.\",\n",
    "        \"clue\": \"Introduces the theme of safety in squatting.\",\n",
    "        \"relevance_to_segment\": \"This earlier emphasis on safety provides context for why proper depth is important and why it's being addressed in our segment. It connects to the fear of knee pain mentioned in LC3.\"\n",
    "      },\n",
    "      {\n",
    "        \"id\": \"GC2\",\n",
    "        \"timestamp\": \"00:02:30\",\n",
    "        \"quote\": \"Squatting is a fundamental movement pattern in everyday life.\",\n",
    "        \"clue\": \"Emphasizes the importance of squats beyond just exercise.\",\n",
    "        \"relevance_to_segment\": \"This broader context heightens the importance of learning proper squat depth as demonstrated in our segment. It suggests that the techniques shown have applications beyond just gym workouts.\"\n",
    "      },\n",
    "      {\n",
    "        \"clue_id\": \"GC3\",\n",
    "        \"timestamp\": \"00:05:20\",\n",
    "        \"quote\": \"If you have existing knee issues, consult a physician before attempting deep squats.\",\n",
    "        \"clue\": \"Provides a health disclaimer related to squat depth.\",\n",
    "        \"relevance_to_segment\": \"While this comes after our segment, it's relevant because it addresses the concern about knee pain mentioned in LC3. It suggests that the demonstration in our segment is generally safe but acknowledges individual variations.\"\n",
    "      },\n",
    "      {\n",
    "        \"clue_id\": \"GC4\",\n",
    "        \"timestamp\": \"00:06:45\",\n",
    "        \"quote\": \"Proper depth ensures full engagement of your quadriceps and glutes.\",\n",
    "        \"clue\": \"Explains the benefit of correct squat depth.\",\n",
    "        \"relevance_to_segment\": \"This later explanation provides justification for the depth guideline given in LC4. It helps viewers understand why the demonstrated technique is important.\"\n",
    "      },\n",
    "      {\n",
    "        \"clue_id\": \"GC5\",\n",
    "        \"timestamp\": \"00:00:30\",\n",
    "        \"quote\": \"Today, we'll cover squat variations for beginners to advanced lifters.\",\n",
    "        \"clue\": \"Outlines the scope of the entire video.\",\n",
    "        \"relevance_to_segment\": \"This early statement suggests that our segment, focusing on proper depth, is part of a comprehensive guide. It implies that the demonstration might be adaptable for different skill levels.\"\n",
    "      }\n",
    "    ]\n",
    "    Double check that the timestamp and the quote that you provide exactly correspond to what you found in the transcript.\n",
    "    For example, if the transcript says:\n",
    "    \"00:05:02\n",
    "    he took the glasses\n",
    "    00:05:04\n",
    "    and gave them to me\"\n",
    "    Then a GOOD output will be:\n",
    "    - timestamp: 00:05:03\n",
    "    - quote: \"he took the glasses and gave them to me\"\n",
    "    And a BAD output would be:\n",
    "    - timestamp: 00:04:02\n",
    "    - quote: \"he gave me the glasses\"\n",
    "    \n",
    "\n",
    "    Good logical inference examples:\n",
    "    [\n",
    "      {\n",
    "        \"id\": \"LI1\",\n",
    "        \"description\": \"Primary Demonstration of Heel Lift\",\n",
    "        \"details\": \"Given that GC1-GC3 describe the 'most common mistake' as heels lifting off the ground, and this description immediately precedes our segment, it's highly probable that this is the primary error being demonstrated. This is further supported by the segment's focus on incorrect form (LC1-LC4).\"\n",
    "      },\n",
    "      {\n",
    "        \"id\": \"LI2\",\n",
    "        \"description\": \"Multiple Error Demonstration\",\n",
    "        \"details\": \"While heel lift is likely the primary focus, the mention of multiple pain points (knees, lower back, shoulders in LC3-LC4) suggests that the demonstrator may be exhibiting several forms of incorrect technique simultaneously. This comprehensive 'what not to do' approach would be pedagogically effective.\"\n",
    "      },\n",
    "      {\n",
    "        \"id\": \"LI3\",\n",
    "        \"description\": \"Possible Inclusion of 'Butt Wink'\",\n",
    "        \"details\": \"Although 'butt wink' is mentioned after our segment (GC4-GC6), its connection to back pain (which is mentioned in LC4) raises the possibility that this error is also present in the demonstration. The instructor may be showing multiple errors early on, then breaking them down individually later.\"\n",
    "      },\n",
    "      {\n",
    "        \"id\": \"LI4\",\n",
    "        \"description\": \"Segment Placement in Overall Video Structure\",\n",
    "        \"details\": \"The segment's position (starting at 00:00:19) and the phrase 'let's see how to do it correctly' (LC5) at the end suggest this is an early, foundational part of the video. It likely serves to grab attention by showing common mistakes before transitioning to proper form instruction.\"\n",
    "      },\n",
    "      {\n",
    "        \"id\": \"LI5\",\n",
    "        \"description\": \"Intentional Exaggeration of Errors\",\n",
    "        \"details\": \"Given the educational nature of the video, it's plausible that the demonstrator is intentionally exaggerating the incorrect form. This would make the errors more obvious to viewers and enhance the contrast with correct form shown later.\"\n",
    "      }\n",
    "    ]\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "GEN_ANNOTATIONS_PROMPT = \"\"\"You are a helpful assistant that performs high quality data investigation and transformation.\n",
    "  You will be given a JSON object with clues and other helpful information about what's going on \n",
    "  in a specific part of a video file. This part is called a segment. Your job is to:\n",
    "  1. Read this JSON object carefully\n",
    "  2. Answer user's questions about this segment\n",
    "  3. Provide the answer as a JSON object in a schema provided by the user\n",
    "  Important rules:\n",
    "  1. You can only rely on data presented in a provided JSON object. Don't improvise.\n",
    "  2. Follow user's request carefully.\n",
    "  3. Don't rush to deliver the answer. Take some time to think. Make a deep breath. Then start writing.\n",
    "  4. If you want to output field as empty (null), output it as JSON null (without quotes), not as a string \"null\". \n",
    "—> GOOD EXAMPLES:\n",
    "  \"wrong\":\"Knees caving in: This can stress the knees and reduce effectiveness\"\n",
    "  \"correction\":\"Focus on keeping knees aligned with your toes.\"\n",
    "  \"wrong\":\"Rounding the back: This increases the risk of back injuries\"\n",
    "  \"correction\":\"Keep your chest up and maintain a neutral spine throughout the movement.\"\n",
    "  \"wrong\":\"Heels are lifting off the ground: this shifts the weight forward, reducing stability\"\n",
    "  \"correction\":\" Keep your weight on your heels and press through them as you rise.\"\n",
    "  \"right\":\"Chest and shoulders: The chest is up, and the shoulders are back, maintaining an upright torso.\"\n",
    "  \"correction\":null\n",
    "—> BAD EXAMPLES:\n",
    "  \"wrong\":\"knees\"\n",
    "  \"correction\":\"fix knees\"\n",
    "  \"wrong\":\"back looks funny\"\n",
    "  \"correction\":\"make back better\"\n",
    "  \"wrong\":\"feet are doing something\"\n",
    "  \"correction\":\"feet should be different\"\n",
    "  \"right\":\"arms\"\n",
    "  \"correction\":\"arms are fine i think\"\n",
    "—> BAD EXAMPLES END HERE\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scrapetube\n",
    "import yt_dlp\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "from collections import defaultdict\n",
    "from datagen.core.sub_utils import vtt_to_txt\n",
    "from datagen.detect_segments import get_segments\n",
    "import torch\n",
    "from transformers import AutoModel, AutoProcessor\n",
    "import pandas as pd\n",
    "from tsmoothie.smoother import LowessSmoother"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import decord\n",
    "import math\n",
    "import numpy as np\n",
    "\n",
    "# decord.bridge.set_bridge(\"torch\")\n",
    "\n",
    "\n",
    "class VideoInferenceDataset(torch.utils.data.IterableDataset):\n",
    "    def __init__(self, video_infos: List[VideoInfo], local_root: Path):\n",
    "        super(VideoInferenceDataset).__init__()\n",
    "\n",
    "        self.video_infos = video_infos\n",
    "        self.local_root = local_root\n",
    "        self.frame_generator = self.get_frame_generator(video_infos, local_root)\n",
    "\n",
    "    @staticmethod\n",
    "    def get_frame_generator(video_infos, local_root: Path):\n",
    "\n",
    "        for video_idx, video_info in enumerate(video_infos):\n",
    "            video_path = local_root.joinpath(video_info.relative_video_path)\n",
    "            vr = decord.VideoReader(str(video_path))\n",
    "            num_frames = len(vr)\n",
    "            fps = vr.get_avg_fps()\n",
    "            frame_indices = range(0, num_frames, round(fps))\n",
    "\n",
    "            for frame_idx in frame_indices:\n",
    "                # print(f\"Frame idx {frame_idx}\")\n",
    "                frame = vr[frame_idx].asnumpy()\n",
    "                yield {\n",
    "                    \"frame\": frame,\n",
    "                    \"frame_idx\": frame_idx,\n",
    "                    \"video_id\": video_idx,\n",
    "                }\n",
    "\n",
    "    def __next__(self):\n",
    "        return next(self.frame_generator)\n",
    "\n",
    "    def __iter__(self):\n",
    "        return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import math\n",
    "\n",
    "# 4. Create nodes\n",
    "\n",
    "\n",
    "def gen_queries_node(state: AgentState):\n",
    "    class QueryList(BaseModel):\n",
    "        \"\"\"A list of queries to find videos on a video hosting service\"\"\"\n",
    "\n",
    "        search_queries: list[str] = Field(default=None, description=\"a list of queries\")\n",
    "\n",
    "    messages = [\n",
    "        SystemMessage(content=str(GEN_QUERIES_PROMPT)),\n",
    "        HumanMessage(content=state[\"task\"]),\n",
    "    ]\n",
    "\n",
    "    model = llm.with_structured_output(QueryList)\n",
    "    response: QueryList = model.invoke(messages)\n",
    "\n",
    "    return {\"search_queries\": response.search_queries[:2]}\n",
    "\n",
    "\n",
    "def get_video_ids_node(state: AgentState):\n",
    "\n",
    "    queries = state[\"search_queries\"]\n",
    "    videos_per_query = 1\n",
    "    sleep = 0\n",
    "    sort_by = \"relevance\"\n",
    "    results_type = \"video\"\n",
    "    only_creative_commons = False\n",
    "\n",
    "    video_ids = set()\n",
    "    for query in queries:\n",
    "        for video in scrapetube.get_search(\n",
    "            query=query,\n",
    "            limit=videos_per_query,\n",
    "            sleep=sleep,\n",
    "            sort_by=sort_by,\n",
    "            results_type=results_type,\n",
    "        ):\n",
    "            video_ids.add(video[\"videoId\"])\n",
    "    video_ids = list(video_ids)\n",
    "\n",
    "    if only_creative_commons:\n",
    "        video_ids_cc = []\n",
    "        for i in video_ids:\n",
    "            YDL_OPTIONS = {\n",
    "                \"quiet\": True,\n",
    "                \"simulate\": True,\n",
    "                \"forceurl\": True,\n",
    "            }\n",
    "            with yt_dlp.YoutubeDL(YDL_OPTIONS) as ydl:\n",
    "                info = ydl.extract_info(f\"youtube.com/watch?v={i}\", download=False)\n",
    "            if \"creative commons\" in info.get(\"license\", \"\").lower():\n",
    "                video_ids_cc.append(i)\n",
    "        video_ids = video_ids_cc\n",
    "\n",
    "    return {\"video_ids\": video_ids}\n",
    "\n",
    "\n",
    "def download_node(state: AgentState):\n",
    "\n",
    "    LOCAL_ROOT = Path(\"./tmp/agent_squats\").resolve()\n",
    "    video_dir = LOCAL_ROOT / \"videos\"\n",
    "    sub_dir = LOCAL_ROOT / \"subs\"\n",
    "\n",
    "    discard_path = LOCAL_ROOT / \"videos_without_subs\"\n",
    "    discard_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    video_ids = state[\"video_ids\"]\n",
    "\n",
    "    downloaded_video_ids = [video_path.stem for video_path in video_dir.glob(\"*.mp4\")]\n",
    "    downloaded_video_ids += [\n",
    "        video_path.stem for video_path in discard_path.glob(\"*.mp4\")\n",
    "    ]\n",
    "\n",
    "    print(f\"Downloaded video ids: {downloaded_video_ids}\")\n",
    "\n",
    "    only_with_transcripts = True\n",
    "\n",
    "    YDL_OPTIONS = {\n",
    "        \"writeautomaticsub\": True,\n",
    "        \"subtitleslangs\": [\"en\"],\n",
    "        \"subtitlesformat\": \"vtt\",\n",
    "        \"overwrites\": False,\n",
    "        \"format\": \"mp4\",\n",
    "        \"outtmpl\": {\n",
    "            \"default\": video_dir.as_posix() + \"/%(id)s.%(ext)s\",\n",
    "            \"subtitle\": sub_dir.as_posix() + \"/%(id)s.%(ext)s\",\n",
    "        },\n",
    "    }\n",
    "\n",
    "    video_infos = []\n",
    "\n",
    "    with yt_dlp.YoutubeDL(YDL_OPTIONS) as ydl:\n",
    "        for video_id in video_ids:\n",
    "            url = f\"https://www.youtube.com/watch?v={video_id}\"\n",
    "\n",
    "            if video_id not in downloaded_video_ids:\n",
    "                try:\n",
    "                    ydl.download(url)\n",
    "                except Exception as e:\n",
    "                    print(datetime.now(), f\"Error at video {video_id}, skipping\")\n",
    "                    print(datetime.now(), e)\n",
    "                    continue\n",
    "\n",
    "            video_path = Path(ydl.prepare_filename({\"id\": video_id, \"ext\": \"mp4\"}))\n",
    "            sub_path = Path(\n",
    "                ydl.prepare_filename(\n",
    "                    {\"id\": video_id, \"ext\": \"en.vtt\"}, dir_type=\"subtitle\"\n",
    "                )\n",
    "            )\n",
    "\n",
    "            with sub_path.open(\"r\") as f:\n",
    "                subs = f.read()\n",
    "\n",
    "            transcript = vtt_to_txt(sub_path)\n",
    "\n",
    "            video_info = VideoInfo(\n",
    "                video_id=video_id,\n",
    "                url=url,\n",
    "                relative_video_path=video_path.relative_to(LOCAL_ROOT).as_posix(),\n",
    "                subs=subs,\n",
    "                transcript=transcript,\n",
    "            )\n",
    "\n",
    "            video_infos.append(video_info)\n",
    "\n",
    "    if only_with_transcripts:\n",
    "        filtered_video_infos = []\n",
    "        for video_info in video_infos:\n",
    "            if video_info.transcript:\n",
    "                filtered_video_infos.append(video_info)\n",
    "            else:\n",
    "                video_path = LOCAL_ROOT / video_info.video_path\n",
    "                video_path.rename(discard_path / video_path.name)\n",
    "        video_infos = filtered_video_infos\n",
    "\n",
    "    return {\"video_infos\": video_infos}\n",
    "\n",
    "\n",
    "def detect_segments_node(state: AgentState):\n",
    "\n",
    "    LOCAL_ROOT = Path(\"./tmp/agent_squats\").resolve()\n",
    "\n",
    "    clip_text_prompts = state[\"clip_text_prompts\"]\n",
    "    video_infos = state[\"video_infos\"]\n",
    "\n",
    "    CLIP_MODEL_ID = \"google/siglip-so400m-patch14-384\"\n",
    "\n",
    "    model = AutoModel.from_pretrained(CLIP_MODEL_ID).to(\"cuda\")\n",
    "    processor = AutoProcessor.from_pretrained(CLIP_MODEL_ID)\n",
    "\n",
    "    dataset = VideoInferenceDataset(video_infos, LOCAL_ROOT)\n",
    "\n",
    "    dataloader = torch.utils.data.DataLoader(\n",
    "        dataset,\n",
    "        num_workers=1,\n",
    "        batch_size=12,\n",
    "        pin_memory=True,\n",
    "        # worker_init_fn=worker_init_fn,\n",
    "    )\n",
    "    dataloader = iter(dataloader)\n",
    "\n",
    "    smoother = LowessSmoother(smooth_fraction=0.02, iterations=1)\n",
    "\n",
    "    clip_results_dict = defaultdict(list)\n",
    "\n",
    "    print(\"Init model complete\")\n",
    "\n",
    "    batch_counter = 0\n",
    "    MAX_BATCHES = 50\n",
    "\n",
    "    while batch_counter < MAX_BATCHES:\n",
    "        batch_counter += 1\n",
    "        try:\n",
    "            start_time = time.time()\n",
    "            batch = next(dataloader)\n",
    "            # print(f\"Fetch time: {time.time() - start_time:.2f} seconds\")\n",
    "        except StopIteration:\n",
    "            break\n",
    "\n",
    "        inputs = processor(\n",
    "            images=batch[\"frame\"],\n",
    "            text=clip_text_prompts,\n",
    "            return_tensors=\"pt\",\n",
    "            padding=True,\n",
    "            truncation=True,\n",
    "        )\n",
    "        inputs = {k: v.to(\"cuda\") for k, v in inputs.items()}\n",
    "\n",
    "        outputs = model(**inputs)\n",
    "\n",
    "        logits = outputs.logits_per_image\n",
    "        probs = torch.nn.functional.sigmoid(logits).detach().cpu().numpy()\n",
    "\n",
    "        for video_idx, frame_idx, prob in zip(\n",
    "            batch[\"video_id\"], batch[\"frame_idx\"], probs\n",
    "        ):\n",
    "            # print(type(video_id.item()), type(frame_idx.item()), type(prob.item()))\n",
    "            video_id = video_infos[video_idx.item()].video_id\n",
    "\n",
    "            clip_results_dict[\"video_id\"].append(video_id)\n",
    "            clip_results_dict[\"frame_idx\"].append(frame_idx.item())\n",
    "            clip_results_dict[\"probs\"].append(prob.item())\n",
    "\n",
    "    print(\"All frames processed\")\n",
    "    clip_results = pd.DataFrame(clip_results_dict)\n",
    "    print(\"Dataframe created\")\n",
    "    print(clip_results)\n",
    "\n",
    "    max_gap_seconds = 1\n",
    "    fps_sampling = 1\n",
    "    min_prob = 0.1\n",
    "    min_segment_seconds = 3\n",
    "    fps = 25\n",
    "\n",
    "    segment_infos = []\n",
    "    for video_id, video_clip_results in clip_results.groupby(\"video_id\"):\n",
    "        probs = video_clip_results[\"probs\"].values\n",
    "        probs = smoother.smooth(probs).smooth_data[0]\n",
    "        segments_start_end = get_segments(\n",
    "            probs,\n",
    "            max_gap=round(max_gap_seconds * fps_sampling),\n",
    "            min_prob=min_prob,\n",
    "            min_segment=round(min_segment_seconds * fps_sampling),\n",
    "        )\n",
    "\n",
    "        print(f\"Segments for video {video_id}: {segments_start_end}\")\n",
    "\n",
    "        sec2ts = lambda s: time.strftime(\n",
    "            f\"%H:%M:%S.{round((s%1)*1000):03d}\", time.gmtime(s)\n",
    "        )\n",
    "\n",
    "        for start, end in segments_start_end:\n",
    "            segment_infos.append(\n",
    "                SegmentInfo(\n",
    "                    start_timestamp=sec2ts(start),\n",
    "                    end_timestamp=sec2ts(end),\n",
    "                    fps=fps,\n",
    "                    video_id=video_id,\n",
    "                )\n",
    "            )\n",
    "\n",
    "    return {\"segment_infos\": segment_infos}\n",
    "\n",
    "\n",
    "def extract_clues_node(state: AgentState):\n",
    "\n",
    "    prompt_template = ChatPromptTemplate.from_messages(\n",
    "        [\n",
    "            (\"system\", EXTRACT_CLUES_PROMPT),\n",
    "            (\n",
    "                \"user\",\n",
    "                \"Segment timecodes: {{ segment_timecodes }}\\nTranscript: {{ transcript }}\",\n",
    "            ),\n",
    "        ],\n",
    "        template_format=\"jinja2\",\n",
    "    )\n",
    "\n",
    "    model = prompt_template | llm.with_structured_output(VideoAnnotation)\n",
    "\n",
    "    segment_infos_dict = defaultdict(list)\n",
    "    for segment_info in state[\"segment_infos\"]:\n",
    "        segment_infos_dict[segment_info.video_id].append(segment_info)\n",
    "\n",
    "    video_infos_dict = {\n",
    "        video_info.video_id: video_info for video_info in state[\"video_infos\"]\n",
    "    }\n",
    "\n",
    "    clues = []\n",
    "\n",
    "    for video_id, segment_infos in segment_infos_dict.items():\n",
    "        transcript = video_infos_dict[video_id].transcript\n",
    "        segment_infos_chunks = [\n",
    "            segment_infos[i : i + 5] for i in range(0, len(segment_infos), 5)\n",
    "        ]\n",
    "\n",
    "        for chunk in segment_infos_chunks:\n",
    "            video_annotation: VideoAnnotation = model.invoke(\n",
    "                {\n",
    "                    \"segment_timecodes\": \"\\n\".join(\n",
    "                        [f\"{s.start_timestamp}-{s.end_timestamp}\" for s in chunk]\n",
    "                    ),\n",
    "                    \"transcript\": transcript,\n",
    "                }\n",
    "            )\n",
    "            clues.extend(video_annotation.segments)\n",
    "\n",
    "    return {\"clues\": clues}\n",
    "\n",
    "\n",
    "def gen_annotations_node(state: AgentState):\n",
    "    class SegmentFeedback(BaseModel):\n",
    "        right: Optional[str] = Field(description=\"what was right in the performance\")\n",
    "        wrong: Optional[str] = Field(description=\"what was wrong in the performance\")\n",
    "        correction: Optional[str] = Field(\n",
    "            description=\"how and in what ways it the performance could be improved\"\n",
    "        )\n",
    "\n",
    "    # The segment timestamps are taken from the provided information.\n",
    "    class SegmentCompleteAnnotation(BaseModel):\n",
    "        squats_probability: Optional[str] = Field(\n",
    "            description=\"how high is the probability that the person is doing squats in the segment: low, medium, high, unknown(null)\"\n",
    "        )\n",
    "        squats_technique_correctness: Optional[str] = Field(\n",
    "            description=\"correctness of the squat technique.\"\n",
    "        )\n",
    "        squats_feedback: Optional[SegmentFeedback] = Field(\n",
    "            description=\"what was right and wrong in the squat perfomance in the segment. When the technique is incorrect, provide instructions how to correct them.\"\n",
    "        )\n",
    "\n",
    "    prompt_template = ChatPromptTemplate.from_messages(\n",
    "        [\n",
    "            (\"system\", GEN_ANNOTATIONS_PROMPT),\n",
    "            (\"user\", \"Clues: {{ clues }}\"),\n",
    "        ],\n",
    "        template_format=\"jinja2\",\n",
    "    )\n",
    "\n",
    "    model = prompt_template | llm.with_structured_output(SegmentCompleteAnnotation)\n",
    "\n",
    "    clues = state[\"clues\"]\n",
    "\n",
    "    annotations = []\n",
    "    for clue in clues:\n",
    "        segment_annotation: SegmentCompleteAnnotation = model.invoke(\n",
    "            {\"clues\": clue.json()}\n",
    "        )\n",
    "\n",
    "        annotations.append(segment_annotation.json())\n",
    "\n",
    "    print(annotations)\n",
    "\n",
    "    return {\"annotations\": annotations}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import StateGraph, END\n",
    "from typing import TypedDict, Annotated, List\n",
    "import operator\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "\n",
    "from langchain_core.messages import AnyMessage, SystemMessage, HumanMessage, AIMessage, ChatMessage\n",
    "\n",
    "memory = MemorySaver()\n",
    "# memory = SqliteSaver.from_conn_string(\":memory:\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "builder = StateGraph(AgentState)\n",
    "\n",
    "builder.add_node(\"generate_queries\", gen_queries_node)\n",
    "builder.add_node(\"get_video_ids\", get_video_ids_node)\n",
    "builder.add_node(\"download\", download_node)\n",
    "builder.add_node(\"detect_segments\", detect_segments_node)\n",
    "builder.add_node(\"extract_clues\", extract_clues_node)\n",
    "builder.add_node(\"gen_annotations\", gen_annotations_node)\n",
    "\n",
    "builder.set_entry_point(\"generate_queries\")\n",
    "\n",
    "# builder.add_conditional_edges(\n",
    "#     \"generate\",\n",
    "#     should_continue,\n",
    "#     {END: END, \"reflect\": \"reflect\"}\n",
    "# )\n",
    "\n",
    "builder.add_edge(\"generate_queries\", \"get_video_ids\")\n",
    "builder.add_edge(\"get_video_ids\", \"download\")\n",
    "builder.add_edge(\"download\", \"detect_segments\")\n",
    "builder.add_edge(\"detect_segments\", \"extract_clues\")\n",
    "builder.add_edge(\"extract_clues\", \"gen_annotations\")\n",
    "builder.add_edge(\"gen_annotations\", END)\n",
    "\n",
    "graph = builder.compile(checkpointer=memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thread = {\"configurable\": {\"thread_id\": \"1\"}}\n",
    "for s in graph.stream(\n",
    "    {\n",
    "        \"task\": \"i wanna teach people how to do squats\",\n",
    "        \"clip_text_prompts\": [\"person doing squats\"],\n",
    "    },\n",
    "    thread,\n",
    "):\n",
    "    if \"download\" in s:\n",
    "        print(\"dowload happened\")\n",
    "    elif \"extract_clues\" in s:\n",
    "        print(\"extract_clues happened\")\n",
    "    else:\n",
    "        print(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph.get_state(thread).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "datagen",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
